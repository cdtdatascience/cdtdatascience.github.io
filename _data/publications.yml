- title: "Coping with Incomplete Data: Recent Advances"
  authors: Marco Console, Paolo Guagliardo, Leonid Libkin, Etienne Toussaint
  year: 2020 
  month:  6
  type: conference 
  published: Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems
  pages: 33-47
  abstract: >
Handling incomplete data in a correct manner is a notoriously hard problem in databases. Theoretical approaches rely on the computationally hard notion of certain answers, while practical solutions rely on ad hoc query evaluation techniques based on three-valued logic. Can we find a middle ground, and produce correct answers efficiently?

- title: "Cryptocurrency Mining Games with Economic Discount and Decreasing Rewards"
  authors: 
  year: 2020
  type: conference 
  published: 37th International Symposium on Theoretical Aspects of Computer Science (STACS 2020)
  publisher: 
  abstract: >
In the consensus protocols used in most cryptocurrencies, participants called miners must find valid blocks of transactions and append them to a shared tree-like data structure. Ideally, the rules of the protocol should ensure that miners maximize their gains if they follow a default strategy, which consists on appending blocks only to the longest branch of the tree, called the blockchain. Our goal is to understand under which circumstances are miners encouraged to follow the default strategy. Unfortunately, most of the existing models work with simplified payoff functions, without considering the possibility that rewards decrease over time because of the game rules (like in Bitcoin), nor integrating the fact that a miner naturally prefers to be paid earlier than later (the economic concept of discount). In order to integrate these factors, we consider a more general model where issues such as economic discount and decreasing rewards can be set as parameters of an infinite stochastic game. In this model, we study the limit situation in which a miner does not receive a full reward for a block if it stops being in the blockchain. We show that if rewards are not decreasing, then miners do not have incentives to create new branches, no matter how high their computational power is. On the other hand, when working with decreasing rewards similar to those in Bitcoin, we show that miners have an incentive to create such branches. Nevertheless, this incentive only occurs when a miner controls a proportion of the computational power which is close to half of the computational power of the entire network.

- title: "Knowledge-Preserving Certain Answers for SQL-like Queries"
  authors: Etienne Toussaint, Paolo Guagliardo, Leonid Libkin
  year: 2020 
  month:  9
  type: conference 
  published: 17th International Conference on Principles of Knowledge Representation and Reasoning {KR-2020}
  pages: 758-767
  publisher: International Joint Conferences on Artificial Intelligence Organization
  abstract: >

- title: "Troubles with Nulls, Views from the Users"
  authors: Etienne Toussaint, Paolo Guagliardo, Leonid Libkin, Juan Sequeda
  year: 2022 
  month:  6
  type: journal 
  published: Proceedings of the VLDB Endowment (PVLDB)
  volume: 15<div class="gsc_oci_field">Issue11
  publisher: Very Large Data Base Endowment Inc.
  abstract: >
Incomplete data, in the form of null values, has been extensively studied since the inception of the relational model in the 1970s. Anecdotally, one hears that the way in which SQL, the standard language for relational databases, handles nulls creates a myriad of problems in everyday applications of database systems. To the best of our knowledge, however, the actual shortcomings of SQL in this respect, as perceived by database practitioners, have not been systematically documented, and it is not known if existing research results can readily be used to address the practical challenges. 

- title: "On the Tractability of Certain Answers for SQL Nulls in Relational Algebra with Inequalities."
  authors: Etienne Toussaint
  year: 2018
  type: conference 
  published: AMW
  abstract: >
Missing values in theoretical models of incomplete database are often represented with marked nulls, while in SQL databases missing values are all denoted by the same syntactic NULL object. Even practical algorithm to approximate certain answers (answers which are true regardless of how incomplete information is interpreted) are often developed in the model with marked nulls. However computing certain answers for marked nulls is co-NP complete even for the most simple queries when inequalities are allowed. In this short paper we study the tractability of certain answers for SQL nulls in a fragment of relational algebra where selection with inequalities is permitted. We define the fragment and present an algorithm to compute certain answers. We also show that if we add even small features to the fragment, computing certain answers becomes intractable. This study emphasises the necessity of a specific certain answers approximation scheme for SQL nulls and offers ideas to design it.

- title: "Flexible Dataset Distillation: Learn Labels Instead of Images"
  authors: Ondrej Bohdal, Yongxin Yang, Timothy Hospedales
  year: 2020 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2006.08572; NeurIPS MetaLearn 2020 Workshop
  abstract: >

- title: "Meta-Calibration: Meta-Learning of Model Calibration Using Differentiable Expected Calibration Error"
  authors: Ondrej Bohdal, Yongxin Yang, Timothy Hospedales
  year: 2021 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2106.09613; ICML 2021 Workshop on Uncertainty and Robustness in Deep Learning
  abstract: >

- title: "A Channel Coding Benchmark for Meta-Learning"
  authors: Rui Li, Ondrej Bohdal, Rajesh Mishra, Hyeji Kim, Da Li, Nicholas Lane, Timothy Hospedales
  year: 2021 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2107.07579; NeurIPS 2021 (Datasets and Benchmarks); AAAI 2021 Workshop on Meta-Learning
  abstract: >

- title: "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization"
  authors: Ondrej Bohdal, Yongxin Yang, Timothy Hospedales
  year: 2021 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2106.10575; NeurIPS 2021; ICML 2021 Beyond First&#8209;Order Methods in ML Systems workshop
  abstract: >
Gradient-based meta-learning and hyperparameter optimization have seen significant progress recently, enabling practical end-to-end training of neural networks together with many hyperparameters. Nevertheless, existing approaches are relatively expensive as they need to compute second-order derivatives and store a longer computational graph. This cost prevents scaling them to larger network architectures. We present EvoGrad, a new approach to meta-learning that draws upon evolutionary techniques to more efficiently compute hypergradients. EvoGrad estimates hypergradient with respect to hyperparameters without calculating second-order gradients, or storing a longer computational graph, leading to significant improvements in efficiency. We evaluate EvoGrad on three substantial recent meta-learning applications, namely cross-domain few-shot learning with feature-wise transformations, noisy label learning with Meta-Weight-Net and low-resource cross-lingual learning with meta representation transformation. The results show that EvoGrad significantly improves efficiency and enables scaling meta-learning to bigger architectures such as from ResNet10 to ResNet34.

<div id="gsc_oci_title">Feed-Forward Source-Free Domain Adaptation via Class Prototypes"
  authors: Ondrej Bohdal, Da Li, Timothy Hospedales
  year: 2022/10
  type: journal 
  published: ECCV 2022 OOD-CV Workshop

- title: "PASHA: Efficient HPO with Progressive Resource Allocation"
  authors: 
  year: 2022 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2207.06940; AutoML Conference 2022 Workshop Track
  abstract: >

- title: "Feed-Forward Source-Free Latent Domain Adaptation via Cross-Attention"
  authors: Ondrej Bohdal, Da Li, Shell Xu Hu, Timothy Hospedales
  year: 2022/7
  type: journal 
  published: arXiv preprint arXiv:2207.07624; ICML 2022 Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward
  abstract: >

<div id="gsc_oci_title">Penalizing Confident Neural Networks"
  authors: Ondrej Bohdal
  year: 2018
  institution: University of Edinburgh

- title: "EXOTica: An Extensible Optimization Toolset for Prototyping and Benchmarking Motion Planning and Control"
  authors: Vladimir Ivan, Yiming Yang, Wolfgang Merkt, Michael P Camilleri, Sethu Vijayakumar
  year: 2019
  type: conference 
  published: Robot Operating System (ROS)
  pages: 211-240
  publisher: Springer, Cham
  abstract: >

- title: "The technology behind a shared demand responsive transport system for a university campus"
  authors: Maria Attard, Michael PJ Camilleri, Adrian Muscat
  year: 2020 
  month:  9
  type: journal 
  published: Research in Transportation Business &amp; Management
  volume: 36
  pages: 100463
  publisher: Elsevier
  abstract: >

- title: "An analytical method of assessment of RemoteFX as a cloud gaming platform"
  authors: Etienne Depasquale, Audrey Zammit, Michael Camilleri, Saviour Zammit, Adrian Muscat, Pierre Mallia, Stefan Scerri
  year: 2014 
  month:  4
  type: conference 
  published: MELECON 2014-2014 17th IEEE Mediterranean Electrotechnical Conference
  pages: 127-133
  publisher: IEEE
  abstract: >

- title: "Autonomous flight control for an RC helicopter"
  authors: Michael Camilleri, Kenneth Scerri, Saviour Zammit
  year: 2012 
  month:  3
  type: conference 
  published: 2012 16th IEEE Mediterranean Electrotechnical Conference
  pages: 391-394
  publisher: IEEE
  abstract: >

- title: "The extended dawid-skene model: Fusing information from multiple data schemas"
  authors: Michael PJ Camilleri, Christopher KI Williams
  year: 2019 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1906.01251
  abstract: >
While label fusion from multiple noisy annotations is a well understood concept in data wrangling (tackled for example by the Dawid-Skene (DS) model), we consider the extended problem of carrying out learning when the labels themselves are not consistently annotated with the same schema. We show that even if annotators use disparate, albeit related, label-sets, we can still draw inferences for the underlying full label-set. We propose the Inter-Schema AdapteR (ISAR) to translate the fully-specified label-set to the one used by each annotator, enabling learning under such heterogeneous schemas, without the need to re-annotate the data. We apply our method to a mouse behavioural dataset, achieving significant gains (compared with DS) in out-of-sample log-likelihood (&#8722;3.40 to &#8722;2.39) and F1-score (0.785 to 0.864).

- title: "Modelling Annotator Variability across Feature Spaces in the Temporal Analysis of Behaviour"
  authors: Michael PJ Camilleri
  year: 2018 
  month:  11
  institution: M. sc. dissertation, University of Edinburgh
  abstract: >
Although behavioural phenotyping is an active area of research in the biological community, there is limited analysis in terms of temporal modelling of behavioural states. Moreover, while most of the data is obtained by human annotators indicating a behaviour from a predefined set of labels (which we call a schema), much of the research ignores the noise inherent in such data.

- title: "Developing the Technology for a Shared Demand Responsive Transport System at the University of Malta"
  authors: Maria Attard, Adrian Muscat, Michael Camilleri
  year: 2018 
  month:  1
  type: conference 
  published: Transportation Research Board 97th Annual Meeting
  abstract: >
Shared Demand Responsive Transport services are flexible services increasingly regarded as an adequate and modern response to meet changing mobility demands. Research provides ample evidence from case studies, as well as the development of technological innovations to cater for and support such services. In Malta, Shared Demand Responsive Transport Services were already found to fill a gap as a mid-market alternative to the private car. The University of Malta is the highest teaching institution and home to a daytime population of 15,000 people, similar in size to any large town in the islands. The University is located at the centre of the island and adjacent to main roads. Complex travel patterns, aggregated in a small area, and restrictions on provision of car parking provided an opportunity for the team to develop the technology for a Shared Demand Responsive Transport System which is tailor-made to the mobility characteristics of the University. This paper provides a background to the case study and describes the development of the technology. Test results carried out at the University of Malta are presented in view of key service level parameters. The study found that the cost of the service is approximately double the cost of local buses, which cost difference is attributed to quality of service improvements. Overall the project demonstrates that ICT enabled demand responsive transport systems are feasible from both a technological and cost point of view. Such systems promise to deliver mobility solutions that compete very well with private car ownership and usage.

<div id="gsc_oci_title">Persistent Object Identification Leveraging Non-Visual Markers"
  authors: Michael PJ Camilleri, Li Zhang, Rasneer S Bains, Andrew Zisserman, Christopher KI Williams
  year: 2021 
  month:  12
  type: journal 
  published: arXiv preprint arXiv:2112.06809

- title: "VJA\&#39;G\&#39;G--A Thick-Client Smart-Phone Journey Detection Algorithm"
  authors: Michael PJ Camilleri, Adrian Muscat, Victor Buttigieg, Maria Attard
  year: 2019 
  month:  8
  type: journal 
  published: arXiv preprint arXiv:1908.10725
  abstract: >

<div id="gsc_oci_title">Automated personal trip identification app for mobility data collection in Malta"
  authors: Maria Attard, Michael Camilleri, Adrian Muscat, Victor Buttigieg
  year: 2017
  type: conference 
  published: NECTAR XIV International Conference on Transport in a Networked Society

- title: "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning"
  authors: Asa Cooper Stickland, Iain Murray
  year: 2019 
  month:  5
  type: conference 
  published: International Conference on Machine Learning
  pages: 5986-5995
  publisher: PMLR
  abstract: >
Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a\hbox {single} BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or &#8216;projected attention layers&#8217;, we match the performance of separately fine-tuned models on the GLUE benchmark with <svg class="gs_fsvg" aria-label="\approx " width="11px" height="6px" style="vertical-align:1px;"><g transform="matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.76200)"><path transform="scale(0.48828, -0.48828)" d="M 143 115  Q 121 115 115 166  V 178  Q 115 271 160 348  T 285 468  T 455 512  Q 565 512 681 441  T 913 300  T 1137 229  Q 1242 229 1330 294  T 1419 461  Q 1425 512 1448 512  Q 1471 512 1477 461  V 449  Q 1477 356 1431 279  T 1306 158  T 1137 115  Q 1051 115 975 150  T 798 254  T 620 360  T 455 397  Q 390 397 323 368  T 214 286  T 172 166  Q 166 115 143 115  Z M 143 592  Q 121 592 115 643  V 655  Q 115 748 160 825  T 285 945  T 455 989  Q 565 989 681 918  T 913 777  T 1137 707  Q 1242 707 1330 772  T 1419 938  Q 1425 989 1448 989  Q 1471 989 1477 938  V 926  Q 1477 833 1431 756  T 1306 635  T 1137 592  Q 1051 592 975 627  T 798 731  T 620 837  T 455 874  Q 390 874 323 845  T 214 763  T 172 643  Q 166 592 143 592  Z "/></g></svg>7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.

- title: "Deep transformers with latent depth"
  authors: Xian Li, Asa Cooper Stickland, Yuqing Tang, Xiang Kong
  year: 2020
  type: journal 
  published: Advances in Neural Information Processing Systems
  volume: 33
  pages: 1736-1746
  abstract: >
The Transformer model has achieved state-of-the-art performance in many sequence modeling tasks. However, how to leverage model capacity with large or variable depths is still an open challenge. We present a probabilistic framework to automatically learn which layer (s) to use by learning the posterior distributions of layer selection. As an extension of this framework, we propose a novel method to train one shared Transformer network for multilingual machine translation with different layer selection posteriors for each language pair. The proposed method alleviates the vanishing gradient issue and enables stable training of deep Transformers (eg 100 layers). We evaluate on WMT English-German machine translation and masked language modeling tasks, where our method outperforms existing approaches for training deeper Transformers. Experiments on multilingual machine translation demonstrate that this approach can effectively leverage increased model capacity and bring universal improvement for both many-to-one and one-to-many translation with diverse language pairs.

- title: "Recipes for adapting pre-trained monolingual and multilingual models to machine translation"
  authors: Asa Cooper Stickland, Xian Li, Marjan Ghazvininejad
  year: 2020 
  month:  4
  type: journal 
  published: arXiv preprint arXiv:2004.14911
  abstract: >

- title: "Diverse ensembles improve calibration"
  authors: Asa Cooper Stickland, Iain Murray
  year: 2020 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2007.04206
  abstract: >

- title: "Multilingual domain adaptation for nmt: Decoupling language and domain information with adapters"
  authors: 
  year: 2021 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2110.09574
  abstract: >

- title: "When does Parameter-Efficient Transfer Learning Work for Machine Translation?"
  authors: 
  year: 2022 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:2205.11277
  abstract: >

- title: "Robustification of Multilingual Language Models to Real-world Noise with Robust Contrastive Pretraining"
  authors: Asa Cooper Stickland, Sailik Sengupta, Jason Krone, Saab Mansour, He He
  year: 2022 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2210.04782
  abstract: >

- title: "Regularising Fisher Information Improves Cross-lingual Generalisation"
  authors: Asa Cooper Stickland, Iain Murray
  year: 2021/11
  type: conference 
  published: Proceedings of the 1st Workshop on Multilingual Representation Learning
  pages: 238-241
  abstract: >
Many recent works use &#8216;consistency regularisation&#8217;to improve the generalisation of fine-tuned pre-trained models, both multilingual and English-only. These works encourage model outputs to be similar between a perturbed and normal version of the input, usually via penalising the Kullback&#8211;Leibler (KL) divergence between the probability distribution of the perturbed and normal model. We believe that consistency losses may be implicitly regularizing the loss landscape. In particular, we build on work hypothesising that implicitly or explicitly regularizing trace of the Fisher Information Matrix (FIM), amplifies the implicit bias of SGD to avoid memorization. Our initial results show both empirically and theoretically that consistency losses are related to the FIM, and show that the flat minima implied by a small trace of the FIM improves performance when fine-tuning a multilingual model on additional languages. We aim to confirm these initial results on more datasets, and use our insights to develop better multilingual fine-tuning techniques.

- title: "BERT and PALs: Projected Attention Layers"
  authors: Asa Cooper Stickland, Iain Murray
  abstract: >

- title: "Neural Spline Flows"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2019
  type: conference 
  published: Advances in Neural Information Processing Systems
  pages: 7509-7520
  abstract: >
A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.

- title: " -- a toolkit for simulation-based inference"
  authors: 
  year: 2020 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2007.09114
  abstract: >

- title: "Maximum Likelihood Training of Score-Based Diffusion Models"
  authors: Yang Song, Conor Durkan, Iain Murray, Stefano Ermon
  year: 2021
  type: conference 
  published: Advances in Neural Information Processing Systems
  abstract: >
Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.

- title: "On Contrastive Learning for Likelihood-Free Inference"
  authors: Conor Durkan, Iain Murray, George Papamakarios
  year: 2020 
  month:  11
  type: conference 
  published: International Conference on Machine Learning
  pages: 2771-2781
  publisher: PMLR
  abstract: >
Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classifier to distinguish between pairs of parameter-observation samples generated using the simulator and pairs sampled from some reference distribution, which implicitly learns a density ratio proportional to the likelihood. Another popular class of methods fits a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of flexible neural density estimators for this task. In this work, we show that both of these approaches can be unified under a general contrastive learning scheme, and clarify how they should be run and compared.

- title: "Cubic-Spline Flows"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2019 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1906.02145
  abstract: >

- title: "Autoregressive Energy Machines"
  authors: Charlie Nash, Conor Durkan
  year: 2019 
  month:  5
  type: conference 
  published: International Conference on Machine Learning
  pages: 1735-1744
  abstract: >
Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.

- title: "Optical ultrafast random number generation at 1 Tb/s using a turbulent semiconductor ring cavity laser"
  authors: T Butler, C Durkan, D Goulding, S Slepneva, B Kelleher, SP Hegarty, G Huyet
  year: 2016 
  month:  1
  type: journal 
  published: Optics letters
  volume: 41<div class="gsc_oci_field">Issue2
  pages: 388-391
  publisher: Optical Society of America
  abstract: >

- title: "nflows: normalizing flows in PyTorch"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2020
  volume: 14
  publisher: Zenodo, v0

- title: "Sequential Neural Methods for Likelihood-Free Inference"
  authors: Conor Durkan, George Papamakarios, Iain Murray
  year: 2018 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1811.08723
  abstract: >

- title: "How Well Do Self-Supervised Models Transfer?"
  authors: Linus Ericsson, Henry Gouk, Timothy M Hospedales
  year: 2021/6
  type: conference 
  published: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  abstract: >
Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.

- title: "Self-Supervised Representation Learning: Introduction, advances, and challenges"
  authors: Linus Ericsson, Henry Gouk, Chen Change Loy, Timothy M Hospedales
  year: 2022 
  month:  5<div class="gsc_oci_field">SourceIEEE Signal Processing Magazine
  volume: 39<div class="gsc_oci_field">Issue3
  pages: 42-62
  publisher: IEEE
  abstract: >

- title: "Why do self-supervised models transfer? investigating the impact of invariance on downstream tasks"
  authors: Linus Ericsson, Henry Gouk, Timothy M Hospedales
  year: 2021 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:2111.11398
  abstract: >

- title: "Region Proposal Network Pre-Training Helps Label-Efficient Object Detection"
  authors: Linus Ericsson, Nanqing Dong, Yongxin Yang, Ales Leonardis, Steven McDonagh
  year: 2022 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:2211.09022
  abstract: >

- title: "BEETLE II: Deep natural language understanding and automatic feedback generation for intelligent tutoring in basic electricity and electronics"
  authors: Myroslava Dzikovska, Natalie Steinhauser, Elaine Farrow, Johanna Moore, Gwendolyn Campbell
  year: 2014/9
  type: journal 
  published: International Journal of Artificial Intelligence in Education
  volume: 24<div class="gsc_oci_field">Issue3
  pages: 284-332
  publisher: Springer New York
  abstract: >

- title: "Pilot randomised controlled trial of Help4Mood, an embodied virtual agent-based system to support treatment of depression"
  authors: Christopher Burton, Aurora Szentagotai Tatar, Brian McKinstry, Colin Matheson, Silviu Matu, Ramona Moldovan, Michele Macnab, Elaine Farrow, Daniel David, Claudia Pagliari, Antoni Serrano Blanco, Maria Wolters, Help4Mood Consortium
  year: 2016/9
  type: journal 
  published: Journal of telemedicine and telecare
  volume: 22<div class="gsc_oci_field">Issue6
  pages: 348-355
  publisher: SAGE Publications
  abstract: >
Help4Mood is an interactive system with an embodied virtual agent (avatar) to assist in self-monitoring of patients receiving treatment for depression. Help4Mood supports self-report and biometric monitoring and includes elements of cognitive behavioural therapy. We aimed to evaluate system use and acceptability, to explore likely recruitment and retention rates in a clinical trial and to obtain an estimate of potential treatment response with a view to conducting a future randomised controlled trial (RCT).<h3 class="gsh_h3">Methods</h3>We conducted a pilot RCT of Help4Mood in three centres, in Romania, Spain and Scotland, UK. Patients with diagnosed depression (major depressive disorder) and current mild/moderate depressive symptoms were randomised to use the system for four weeks in addition to treatment as usual (TAU) or to TAU alone.<h3 class="gsh_h3">Results</h3>

- title: "Beetle II: a system for tutoring and computational linguistics experimentation"
  authors: Myroslava O Dzikovska, Johanna D Moore, Natalie Steinhauser, Gwendolyn Campbell, Elaine Farrow, Charles B Callaway
  year: 2010/7
  type: conference 
  published: Proceedings of the ACL 2010 System Demonstrations
  pages: 13-18
  abstract: >
We present BEETLE II, a tutorial dialogue system designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutorial policies and demonstrated that the system can be successfully used to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of natural language interpretation and generation techniques.

- title: "Using natural language processing to analyze tutorial dialogue corpora across domains modalities"
  authors: Diane Litman, Johanna Moore, Myroslava O Dzikovska, Elaine Farrow
  year: 2009
  type: conference 
  published: Artificial Intelligence in Education
  pages: 149-156
  publisher: IOS Press
  abstract: >
Our research goal is to investigate whether previous findings and methods in the area of tutorial dialogue can be generalized across dialogue corpora that differ in domain (mechanics versus electricity in physics), modality (spoken versus typed), and tutor type (computer versus human). We first present methods for unifying our prior coding and analysis methods. We then show that many of our prior findings regarding student dialogue behaviors and learning not only generalize across corpora, but that our methodology yields additional new findings. Finally, we show that natural language processing can be used to automate some of these analyses.

- title: "Analysing discussion forum data: a replication study avoiding data contamination"
  authors: Elaine Farrow, Johanna Moore, Dragan Ga&#353;evi&#263;
  year: 2019 
  month:  3
  type: conference 
  published: Proceedings of the 9th international conference on learning analytics &amp; knowledge
  pages: 170-179
  abstract: >
The widespread use of online discussion forums in educational settings provides a rich source of data for researchers interested in how collaboration and interaction can foster effective learning. Such online behaviour can be understood through the Community of Inquiry framework, and the cognitive presence construct in particular can be used to characterise the depth of a student&#39;s critical engagement with course material. Automated methods have been developed to support this task, but many studies used small data sets, and there have been few replication studies.

- title: " System"
  authors: Myroslava O Dzikovska, Diana Bental, Johanna D Moore, Natalie B Steinhauser, Gwendolyn E Campbell, Elaine Farrow, Charles B Callaway
  year: 2010 
  month:  9
  type: conference 
  published: European Conference on Technology Enhanced Learning
  pages: 620-625
  publisher: Springer, Berlin, Heidelberg
  abstract: >
We present <span class="gs_fscp">Beetle II</span>, a tutorial dialogue system designed to accept unrestricted language input and support experimentation with different tutorial planning and dialogue strategies. Our first system evaluation used two different tutoring policies and demonstrated that <span class="gs_fscp">Beetle II</span> can be successfully used as a platform to study the impact of different approaches to tutoring. In the future, the system can also be used to experiment with a variety of parameters that may affect learning in intelligent tutoring systems.

- title: "Dealing with interpretation errors in tutorial dialogue"
  authors: Myroslava O Dzikovska, Charles B Callaway, Elaine Farrow, Johanna D Moore, Natalie Steinhauser, Gwendolyn Campbell
  year: 2009/9
  type: conference 
  published: Proceedings of the SIGDIAL 2009 Conference
  pages: 38-45
  abstract: >
We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system.

- title: "The Beetle and BeeDiff tutoring systems"
  authors: Charles Callaway, Myroslava Dzikovska, Elaine Farrow, Manuel Marques-Pita, Colin Matheson, Johanna D Moore
  year: 2007
  abstract: >

- title: "Dialogue attributes that inform depth and quality of participation in course discussion forums"
  authors: Elaine Farrow, Johanna Moore, Dragan Ga&#353;evi&#263;
  year: 2020 
  month:  3
  type: conference 
  published: Proceedings of the tenth international conference on learning analytics &amp; knowledge
  pages: 129-134
  abstract: >
This paper describes work in progress to answer the question of how we can identify and model the depth and quality of student participation in class discussion forums using the content of the discussion forum messages. We look at two widely-studied frameworks for assessing critical discourse and cognitive engagement: the ICAP and Community of Inquiry (CoI) frameworks. Our goal is to discover where they agree and where they offer complementary perspectives on learning.

- title: "Diagnosing Natural Language Answers to Support Adaptive Tutoring."
  authors: Myroslava O Dzikovska, Gwendolyn E Campbell, Charles B Callaway, Natalie B Steinhauser, Elaine Farrow, Johanna D Moore, Leslie A Butler, Colin Matheson
  year: 2008/5
  type: conference 
  published: FLAIRS Conference
  pages: 403-408
  abstract: >
Understanding answers to open-ended explanation questions is important in intelligent tutoring systems. Existing systems use natural language techniques in essay analysis, but revert to scripted interaction with short-answer questions during remediation, making adapting dialogue to individual students difficult. We describe a corpus study that shows that there is a relationship between the types of faulty answers and the remediation strategies that tutors use; that human tutors respond differently to different kinds of correct answers; and that re-stating correct answers is associated with improved learning. We describe a design for a diagnoser based on this study that supports remediation in open-ended questions and provides an analysis of natural language answers that enables adaptive generation of tutorial feedback for both correct and faulty answers.

- title: "&quot; Why is the Doctor a Man&quot; Reactions of Older Adults to a Virtual Training Doctor"
  authors: Aurora Constantin, Catherine Lai, Elaine Farrow, Beatrice Alex, Ruth Pel-Littel, Henk Herman Nap, Johan Jeuring
  year: 2019 
  month:  5
  type: conference 
  published: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
  pages: 1-6
  abstract: >

- title: "Combining semantic interpretation and statistical classification for improved explanation processing in a tutorial dialogue system"
  authors: Myroslava O Dzikovska, Elaine Farrow, Johanna D Moore
  year: 2013 
  month:  7<div class="gsc_oci_field">SourceInternational Conference on Artificial Intelligence in Education
  pages: 279-288
  publisher: Springer, Berlin, Heidelberg
  abstract: >
We present an approach for combining symbolic interpretation and statistical classification in the natural language processing (NLP) component of a tutorial dialogue system. Symbolic NLP approaches support dynamic generation of context-adaptive natural language feedback, but lack robustness. In contrast, statistical classification approaches are robust to ill-formed input but provide less detail for context-specific feedback generation. We describe a system design that combines symbolic interpretation with statistical classification to support context-adaptive, dynamically generated natural language feedback, and show that the combined system significantly improves interpretation quality while retaining the adaptivity benefits of a symbolic interpreter.

- title: "IDEAL Household Energy Dataset"
  authors: Nigel Goddard, Jonathan Kilgour, Martin Pullinger, DK Arvind, Heather Lovell, Johanna Moore, David Shipworth, Charles Sutton, Jan Webb, Niklas Berliner, Cillian Brewitt, Myroslava Dzikovska, Edmund Farrow, Elaine Farrow, Janek Mann, Evan Morgan, Lynda Webb, Mingjun Zhong
  year: 2020 
  month:  5
  publisher: University of Edinburgh. School of Informatics
  abstract: >

- title: "The DeMAND coding scheme: A common language for representing and analyzing student discourse"
  authors: Gwendolyn E Campbell, Natalie B Steinhauser, Myroslava O Dzikovska, Johanna D Moore, Charles B Callaway, Elaine Farrow
  year: 2010 
  month:  1
  publisher: NAVAL AIR WARFARE CENTER TRAINING SYSTEMS DIV ORLANDO FL
  abstract: >
We propose that a set of five dimensions forms a foundation underlying a number of prevalent theoretical perspectives on learning. We show how student contributions to instructional dialogue can be reliably annotated with these dimensions. Finally, we provide preliminary validation evidence for our coding scheme and illustrate the potential value of such an approach to analyzing student behavior in tutorial dialogue.Descriptors:

- title: "A network analytic approach to integrating multiple quality measures for asynchronous online discussions"
  authors: Elaine Farrow, Johanna Moore, Dragan Gasevic
  year: 2021 
  month:  4
  type: conference 
  published: LAK21: 11th International Learning Analytics and Knowledge Conference
  pages: 248-258
  abstract: >

- title: "A life story in three parts: the use of triptychs to make sense of personal digital data"
  authors: Lisa Thomas, Elaine Farrow, Matthew Aylett, Pam Briggs
  year: 2018/8
  type: journal 
  published: Personal and Ubiquitous Computing
  volume: 22<div class="gsc_oci_field">Issue4
  pages: 691-705
  publisher: Springer London
  abstract: >

- title: "Adaptive tutorial dialogue systems using deep NLP techniques"
  authors: Myroslava O Dzikovska, Charles B Callaway, Elaine Farrow, Manuel Marques-Pita, Colin Matheson, Johanna D Moore
  year: 2007/4
  type: conference 
  published: Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)
  pages: 5-6
  abstract: >
We present tutorial dialogue systems in two different domains that demonstrate the use of dialogue management and deep natural language processing techniques. Generation techniques are used to produce natural sounding feedback adapted to student performance and the dialogue history, and context is used to interpret tentative answers phrased as questions.

- title: "Generating narratives from personal digital data: using sentiment, themes, and named entities to construct stories"
  authors: Elaine Farrow, Thomas Dickinson, Matthew P Aylett
  year: 2015 
  month:  9
  type: conference 
  published: IFIP Conference on Human-Computer Interaction
  pages: 473-477
  publisher: Springer, Cham
  abstract: >
As the quantity and variety of personal digital data shared on social media continues to grow, how can users make sense of it? There is growing interest among HCI researchers in using narrative techniques to support interpretation and understanding. This work describes our prototype application, ReelOut, which uses narrative techniques to allow users to understand their data as more than just a database. The online service extracts data from multiple social media sources and augments it with semantic information such as sentiment, themes, and named entities. The interactive editor automatically constructs a story by using unit selection to fit data units to a simple narrative structure. It allows the user to change the story interactively by rejecting certain units or selecting a new narrative target. Finally, images from the story can be exported as a video clip or a collage.

- title: "Metacognitive awareness versus linguistic politeness: Expressions of confusion in tutorial dialogues"
  authors: Gwendolyn E Campbell, Natalie B Steinhauser, Myroslava Dzikovska, Johanna D Moore, Charles B Callaway, Elaine Farrow
  year: 2009 
  month:  1
  publisher: NAVAL AIR WARFARE CENTER TRAINING SYSTEMS DIV ORLANDO FL
  abstract: >
Research suggests that students who are aware of their own confusions and take steps to resolve those confusions are most likely to benefit from a learning experience. At the same time, there are conversational maxims, such as Leechs politeness maxims, that may inhibit a student from expressing and pursuing confusions within a tutorial dialogue. We investigated students expressions of confusion while working through a series of learning activities with a tutor. We found that, during the times when students were working independently on an activity, their expressions of confusion were reliable indicators of their lack of understanding however, when they were conversing with their tutors, these same students did not express confusion and, in fact, the more often the expressed comprehension, the worse they performed on the post-test. This suggests that student metacognitive statements should not be interpreted without taking into consideration the context in which they were expressed. We briefly consider implications for human tutors and the development of computer tutoring systems.Descriptors:

- title: "Interpretation and Generation in a Knowledge-Based TutorialSystem"
  authors: Myroslava O Dzikovska, Charles B Callaway, Elaine Farrow
  year: 2006
  type: conference 
  published: Proceedings of the Workshop KRAQ&#8217;06: Knowledge and Reasoning for Language Processing
  abstract: >
We discuss how deep interpretation and generation can be integrated with a knowledge representation designed for question answering to build a tutorial dialogue system. We use a knowledge representation known to perform well in answering exam-type questions and show that to support tutorial dialogue it needs additional features, in particular, compositional representations for interpretation and structured explanation representations.

- title: "Optimising Placement of Pollution Sensors in Windy Environments"
  authors: Sigrid Passano Hellan, Christopher G Lucas, Nigel H Goddard
  year: 2020/12
  type: journal 
  published: arXiv e-prints
  pages: arXiv: 2012.10770
  abstract: >
Air pollution is one of the most important causes of mortality in the world. Monitoring air pollution is useful to learn more about the link between health and pollutants, and to identify areas for intervention. Such monitoring is expensive, so it is important to place sensors as efficiently as possible. Bayesian optimisation has proven useful in choosing sensor locations, but typically relies on kernel functions that neglect the statistical structure of air pollution, such as the tendency of pollution to propagate in the prevailing wind direction. We describe two new wind-informed kernels and investigate their advantage for the task of actively learning locations of maximum pollution using Bayesian optimisation.

- title: "Bayesian Optimisation for Active Monitoring of Air Pollution"
  authors: Sigrid Passano Hellan, Christopher G Lucas, Nigel H Goddard
  year: 2022/2
  type: journal 
  published: arXiv e-prints
  pages: arXiv: 2202.07595
  abstract: >
Air pollution is one of the leading causes of mortality globally, resulting in millions of deaths each year. Efficient monitoring is important to measure exposure and enforce legal limits. New low-cost sensors can be deployed in greater numbers and in more varied locations, motivating the problem of efficient automated placement. Previous work suggests Bayesian optimisation is an appropriate method, but only considered a satellite data set, with data aggregated over all altitudes. It is ground-level pollution, that humans breathe, which matters most. We improve on those results using hierarchical models and evaluate our models on urban pollution data in London to show that Bayesian optimisation can be successfully applied to the problem.

- title: "Poetic rhyme reflects cross-linguistic differences in information structure"
  authors: Michael Wagner, Katherine McCurdy
  year: 2010 
  month:  11
  type: journal 
  published: Cognition
  volume: 117<div class="gsc_oci_field">Issue2
  pages: 166-175
  publisher: Elsevier
  abstract: >

- title: "Inflecting When There&#8217;s No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals"
  authors: Kate McCurdy, Sharon Goldwater, Adam Lopez
  year: 2020
  type: conference 
  published: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
  pages: 1745&#8211;1756
  abstract: >

- title: "Grammatical gender associations outweigh topical gender bias in crosslinguistic word embeddings"
  authors: Katherine McCurdy, Oguz Serbetci
  year: 2017
  type: conference 
  published: Widening Natural Language Processing (WiNLP)
  abstract: >

- title: "Implicit prosody and contextual bias in silent reading"
  authors: Kate McCurdy, Gerrit Kentner, Shravan Vasishth
  year: 2013 
  month:  7
  type: journal 
  published: Journal of Eye Movement Research
  volume: 6<div class="gsc_oci_field">Issue2
  abstract: >

- title: "Tutorbot Corpus: Evidence of Human-Agent Verbal Alignment in Second Language Learner Dialogues."
  authors: Arabella Sinclair, Kate McCurdy, Christopher G Lucas, Adam Lopez, Dragan Ga&#353;evic
  year: 2019/7
  type: journal 
  published: International Educational Data Mining Society
  publisher: International Educational Data Mining Society
  abstract: >
Prior research has shown that, under certain conditions, Human-Agent (HA) alignment exists to a stronger degree than that found in Human-Human (HH) communication. In an HH Second Language (L2) setting, evidence of alignment has been linked to learning and teaching strategy. We present a novel analysis of HA and HH L2 learner dialogues using automated metrics of alignment. Our contributions are twofold: firstly we replicated the reported HA alignment within an educational context, finding L2 students align to an automated tutor. Secondly, we performed an exploratory comparison of the alignment present in comparable HA and HH L2 learner corpora using Bayesian Gaussian Mixture Models (GMMs), finding preliminary evidence that students in HA L2 dialogues showed greater variability in engagement.

- title: "Generalising to German Plural Noun Classes, from the Perspective of a Recurrent Neural Network"
  authors: Verna Dankers, Anna Langedijk, Kate McCurdy, Adina Williams, Dieuwke Hupkes
  year: 2021/11
  type: conference 
  published: Proceedings of the 25th Conference on Computational Natural Language Learning
  pages: 94-108
  abstract: >
Inflectional morphology has since long been a useful testing ground for broader questions about generalisation in language and the viability of neural network models as cognitive models of language. Here, in line with that tradition, we explore how recurrent neural networks acquire the complex German plural system and reflect upon how their strategy compares to human generalisation and rule-based models of this system. We perform analyses including behavioural experiments, diagnostic classification, representation analysis and causal interventions, suggesting that the models rely on features that are also key predictors in rule-based models of German plurals. However, the models also display shortcut learning, which is crucial to overcome in search of more cognitively plausible generalisation behaviour.

- title: "Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection"
  authors: Kate McCurdy, Adam Lopez, Sharon Goldwater
  year: 2020/11
  type: conference 
  published: Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics
  pages: 59-65
  abstract: >
Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.

- title: "Adaptor grammars for unsupervised paradigm clustering"
  authors: Kate McCurdy, Sharon Goldwater, Adam Lopez
  year: 2021/8
  type: conference 
  published: Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology
  pages: 82-89
  abstract: >
This work describes the Edinburgh submission to the SIGMORPHON 2021 Shared Task 2 on unsupervised morphological paradigm clustering. Given raw text input, the task was to assign each token to a cluster with other tokens from the same paradigm. We use Adaptor Grammar segmentations combined with frequency-based heuristics to predict paradigm clusters. Our system achieved the highest average F1 score across 9 test languages, placing first out of 15 submissions.

- title: "Regularization or lexical probability-matching? How German speakers generalize plural morphology"
  authors: Kate McCurdy, Sharon Goldwater, Adam Lopez
  year: 2022 
  month:  4
  type: conference 
  published: 44th Annual Meeting of the Cognitive Science Society
  abstract: >
Artificial language learning research has shown that, under some conditions, adult speakers tend to probability-match to inconsistent variation in their input, while in others, they regularize by reducing that variation. We demonstrate that this framework can characterize speaker behavior in a naturallanguage morphological inflection task: the lexicon can be used to estimate variation in speaker productions. In the task of German plural inflection, we find that speakers probabilitymatch a lexical distribution conditioned on phonology, and largely disregard an alternative possible strategy of conditional regularization based on grammatical gender.

- title: "Modeling grammatical gender and plural inflection in German"
  authors: Kate McCurdy, Adam Lopez, Sharon Goldwater
  year: 2020 
  month:  9
  type: conference 
  published: 26th Architectures and Mechanisms for Language Processing Conference
  abstract: >

- title: "Semeval 2021 task 7: Hahackathon, detecting and rating humor and offense"
  authors: JA Meaney, Steven Wilson, Luis Chiruzzo, Adam Lopez, Walid Magdy
  year: 2021/8
  type: conference 
  published: Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)
  pages: 105-119
  abstract: >
SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which models excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best systems.

- title: "Overview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish"
  authors: 
  year: 2021 
  month:  9
  type: journal 
  published: Procesamiento del Lenguaje Natural
  volume: 67
  pages: 257-268
  abstract: >
We present the results of HAHA at IberLEF 2021: Humor Analysis ba-sed on Human Annotation. This year&#8217;s edition of the competition includes the two classic tasks of humor detection and rating, plus two novel tasks of humor logic me-chanism and target classi&#64257;cation. We describe the corpus created for the challenge, the competition phases, the submitted systems and the main results obtained.

- title: "Consistent use of proactive control and relation with academic achievement in childhood"
  authors: 
  year: 2020 
  month:  10
  type: journal 
  published: Cognition
  volume: 203
  pages: 104329
  publisher: Elsevier
  abstract: >

- title: "Adaptiveness in proactive control engagement in children and adults"
  authors: Nicolas Chevalier, Julie Anne Meaney, Hilary Joy Traut, Yuko Munakata
  year: 2020 
  month:  12
  type: journal 
  published: Developmental cognitive neuroscience
  volume: 46
  pages: 100870
  publisher: Elsevier
  abstract: >

- title: "The effects of verbal and spatial memory load on children&#39;s processing speed"
  authors: 
  year: 2018/7
  type: journal 
  published: Annals of the New York Academy of Sciences
  volume: 1424<div class="gsc_oci_field">Issue1
  pages: 161-174
  abstract: >

- title: "Age-related differentiation in verbal and visuospatial working memory processing in childhood"
  authors: 
  year: 2020/11
  type: journal 
  published: Psychological Research
  volume: 84<div class="gsc_oci_field">Issue8
  pages: 2354-2360
  publisher: Springer Berlin Heidelberg
  abstract: >

- title: "Crossing the Line: Where do Demographic Variables Fit into Humor Detection?"
  authors: JA Meaney
  year: 2020/7
  type: conference 
  published: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop
  pages: 176-181
  abstract: >
Recent humor classification shared tasks have struggled with two issues: either the data comprises a highly constrained genre of humor which does not broadly represent humor, or the data is so indiscriminate that the inter-annotator agreement on its humor content is drastically low. These tasks typically average over all annotators&#8217; judgments, in spite of the fact that humor is a highly subjective phenomenon. We argue that demographic factors influence whether a text is perceived as humorous or not. We propose the addition of demographic information about the humor annotators in order to bin ratings more sensibly. We also suggest the addition of an &#8216;offensive&#8217;label to distinguish between different generations, in terms of humor. This would allow for more nuanced shared tasks and could lead to better performance on downstream tasks, such as content moderation.

- title: "Smash at SemEval-2020 task 7: Optimizing the hyperparameters of ERNIE 2.0 for humor ranking and rating"
  authors: JA Meaney, Steven Wilson, Walid Magdy
  year: 2020/12
  type: conference 
  published: Proceedings of the Fourteenth Workshop on Semantic Evaluation
  pages: 1049-1054
  abstract: >
The use of pre-trained language models such as BERT and ULMFiT has become increasingly popular in shared tasks, due to their powerful language modelling capabilities. Our entry to SemEval uses ERNIE 2.0, a language model which is pre-trained on a large number of tasks to enrich the semantic and syntactic information learned. ERNIE&#8217;s knowledge masking pre-training task is a unique method for learning about named entities, and we hypothesise that it may be of use in a dataset which is built on news headlines and which contains many named entities. We optimize the hyperparameters in a regression and classification model and find that the hyperparameters we selected helped to make bigger gains in the classification model than the regression model.

- title: "Don&#8217;t Take It Personally: Analyzing Gender and Age Differences in Ratings of Online Humor"
  authors: JA Meaney, Steven R Wilson, Luis Chiruzzo, Walid Magdy
  year: 2022
  type: conference 
  published: International Conference on Social Informatics
  pages: 20-33
  publisher: Springer, Cham
  abstract: >
Computational humor detection systems rarely model the subjectivity of humor responses, or consider alternative reactions to humor - namely offense. We analyzed a large dataset of humor and offense ratings by male and female annotators of different age groups. We find that women link these two concepts more strongly than men, and they tend to give lower humor ratings and higher offense scores. We also find that the correlation between humor and offense increases with age. Although there were no gender or age differences in humor detection, women and older annotators signalled that they did not understand joke texts more often than men. We discuss implications for computational humor detection and downstream tasks.

"
  authors: 
  year: 2021
  type: journal 
  published: Procesamiento del lenguaje natural<div class="gsc_oci_field">Issue67
  pages: 257-268
  publisher: 
  abstract: >

- title: "Metalinguistic Awareness and Executive Functions in Irish-English Bilinguals"
  authors: Julie-Anne Meaney
  year: 2016
  publisher: The University of Edinburgh
  abstract: >

- title: "Meta-learning in neural networks: A survey"
  authors: Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey
  year: 2020 
  month:  4
  type: journal 
  published: IEEE Transactions on Pattern Analysis and Machine Intelligence
  abstract: >

- title: "Zero-shot knowledge transfer via adversarial belief matching"
  authors: Paul Micaelli, Amos Storkey
  year: 2019 
  month:  5
  type: conference 
  published: Advances in Neural Information Processing Systems
  volume: 32
  abstract: >
Performing knowledge transfer from a large teacher network to a smaller student is a popular task in modern deep learning applications. However, due to growing dataset sizes and stricter privacy regulations, it is increasingly common not to have access to the data that was used to train the teacher. We propose a novel method which trains a student to match the predictions of its teacher without using any data or metadata. We achieve this by training an adversarial generator to search for images on which the student poorly matches the teacher, and then using them to train the student. Our resulting student closely approximates its teacher for simple datasets like SVHN, and on CIFAR10 we improve on the state-of-the-art for few-shot distillation (with 100 images per class), despite using no data. Finally, we also propose a metric to quantify the degree of belief matching between teacher and student in the vicinity of decision boundaries, and observe a significantly higher match between our zero-shot student and the teacher, than between a student distilled with real data and the teacher. 

- title: "Accurate prediction of X-ray pulse properties from a free-electron laser using machine learning"
  authors: 
  year: 2017 
  month:  6
  type: journal 
  published: Nature communications
  volume: 8<div class="gsc_oci_field">Issue1
  pages: 1-9
  publisher: Nature Publishing Group
  abstract: >

- title: "Meta-learning in neural networks: A survey. arXiv 2020"
  authors: T Hospedales, A Antoniou, P Micaelli, A Storkey
  type: journal 
  published: arXiv preprint arXiv:2004.05439

- title: "Gradient-based Hyperparameter Optimization Over Long Horizons"
  authors: Paul Micaelli, Amos J Storkey
  year: 2021 
  month:  12
  type: conference 
  published: Advances in Neural Information Processing Systems
  volume: 34
  abstract: >
Gradient-based hyperparameter optimization has earned a widespread popularity in the context of few-shot meta-learning, but remains broadly impractical for tasks with long horizons (many gradient steps), due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a significant performance drop. We propose forward-mode differentiation with sharing (FDS), a simple and efficient algorithm which tackles memory scaling issues with forward-mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. We provide theoretical guarantees about the noise reduction properties of our algorithm, and demonstrate its efficiency empirically by differentiating through 10^4 speedups compared to the state-of-the-art black-box methods.

- title: "Machine learning applied to single-shot x-ray diagnostics in an XFEL"
  authors: 
  year: 2016 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1610.03378
  abstract: >

- title: "iSarcasm: A dataset of intended sarcasm"
  authors: Silviu Oprea, Walid Magdy
  year: 2019 
  month:  11
  type: conference 
  published: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
  abstract: >

- title: "SemEval-2022 task 6: iSarcasmEval, intended sarcasm detection in English and Arabic"
  authors: Ibrahim Abu Farha, Silviu Vlad Oprea, Steven Wilson, Walid Magdy
  year: 2022/7
  type: conference 
  published: Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)
  pages: 802-814
  abstract: >
"iSarcasmEval is the first shared task to target intended sarcasm detection: the data for this task was provided and labelled by the authors of the texts themselves. Such an approach minimises the downfalls of other methods to collect sarcasm data, which rely on distant supervision or third-party annotations. The shared task contains two languages, English and Arabic, and three subtasks: sarcasm detection, sarcasm category classification, and pairwise sarcasm identification given a sarcastic sentence and its non-sarcastic rephrase. The task received submissions from 60 different teams, with the sarcasm detection task being the most popular. Most of the participating teams utilised pre-trained language models. In this paper, we provide an overview of the task, data, and participating teams."

- title: "Towards global flood mapping onboard low cost satellites with machine learning"
  authors: 
  year: 2021 
  month:  3
  type: journal 
  published: Scientific reports
  volume: 11<div class="gsc_oci_field">Issue1
  pages: 1-12
  publisher: Nature Publishing Group
  abstract: >

- title: "Exploring author context for detecting intended vs perceived sarcasm"
  authors: Silviu Oprea, Walid Magdy
  year: 2019 
  month:  10
  type: conference 
  published: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
  abstract: >

- title: "The effect of sociocultural variables on sarcasm communication online"
  authors: Silviu Vlad Oprea, Walid Magdy
  year: 2020 
  month:  5
  type: journal 
  published: Proceedings of the ACM on Human-Computer Interaction
  volume: 4<div class="gsc_oci_field">IssueCSCW1
  pages: 1-22
  publisher: ACM
  abstract: >

- title: "Chandler: An Explainable Sarcastic Response Generator"
  authors: Silviu Oprea, Steven Wilson, Walid Magdy
  year: 2021/11
  type: conference 
  published: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations
  pages: 339-349
  abstract: >
We introduce Chandler, a system that generates sarcastic responses to a given utterance. Previous sarcasm generators assume the intended meaning that sarcasm conceals is the opposite of the literal meaning. We argue that this traditional theory of sarcasm provides a grounding that is neither necessary, nor sufficient, for sarcasm to occur. Instead, we ground our generation process on a formal theory that specifies conditions that unambiguously differentiate sarcasm from non-sarcasm. Furthermore, Chandler not only generates sarcastic responses, but also explanations for why each response is sarcastic. This provides accountability, crucial for avoiding miscommunication between humans and conversational agents, particularly considering that sarcastic communication can be offensive. In human evaluation, Chandler achieves comparable or higher sarcasm scores, compared to state-of-the-art generators, while generating more diverse responses, that are more specific and more coherent to the input.

- title: "Unsupervised Word Translation Pairing using Refinement based Point Set Registration"
  authors: Silviu Oprea, Sourav Dutta, Haytham Assem
  year: 2020 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:2011.13200
  abstract: >

- title: "E-books and Graphics with LaTeXML"
  authors: Deyan Ginev, Bruce R Miller, Silviu Oprea
  year: 2014 
  month:  7
  type: conference 
  published: International Conference on Intelligent Computer Mathematics
  pages: 427-430
  publisher: Springer, Cham
  abstract: >
Marked by the highlights of native generation of epubE-books and TikZ support for creating svg images, we present an annual report of LATExml development in 2013. LATExml provides a reimplementation of the TEX parser, geared towards preserving macro semantics; it supports an array of output formats, notably html5, epub, xhtml and its own LATEX-near xml. Other highlights include enhancing performance when used inside high-throughput build-systems, via incorporating a native zip archive workflow, as well as a simplified installation procedure that now allows to deploy LaTeXML as a cloud service. To this end, we also introduce an official plugin-based scheme for publishing new features that go beyond the core scope of LaTeXML, such as web services or unconventional postprocessors. The software suite has now migrated to GitHub and we welcome forks and patches from the wider FLOSS community.

- title: "Multi-Stage Framework with Refinement Based Point Set Registration for Unsupervised Bi-Lingual Word Alignment"
  authors: Silviu Vlad Oprea, Sourav Dutta, Haytham Assem
  year: 2022/10
  type: conference 
  published: Proceedings of the 29th International Conference on Computational Linguistics
  pages: 1089-1097
  abstract: >
Cross-lingual alignment of word embeddings are important in knowledge transfer across languages, for improving machine translation and other multi-lingual applications. Current unsupervised approaches relying on learning structure-preserving transformations, using adversarial networks and refinement strategies, suffer from instability and convergence issues. This paper proposes BioSpere, a novel multi-stage framework for unsupervised mapping of bi-lingual word embeddings onto a shared vector space, by combining adversarial initialization, refinement procedure and point set registration. Experiments for parallel dictionary induction and word similarity demonstrate state-of-the-art unsupervised results for BioSpere on diverse languages&#8211;showcasing robustness against variable adversarial performance.

- title: "Should a Chatbot be Sarcastic? Understanding User Preferences Towards Sarcasm Generation"
  authors: Silviu Vlad Oprea, Steven Wilson, Walid Magdy
  year: 2022/5
  type: conference 
  published: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
  pages: 7686-7700
  abstract: >
Previous sarcasm generation research has focused on how to generate text that people perceive as sarcastic to create more human-like interactions. In this paper, we argue that we should first turn our attention to the question of when sarcasm should be generated, finding that humans consider sarcastic responses inappropriate to many input utterances. Next, we use a theory-driven framework for generating sarcastic responses, which allows us to control the linguistic devices included during generation. For each device, we investigate how much humans associate it with sarcasm, finding that pragmatic insincerity and emotional markers are devices crucial for making sarcasm recognisable.

- title: "Telescoping Density-Ratio Estimation"
  authors: Benjamin Rhodes, Kai Xu, Michael U Gutmann
  year: 2020
  type: journal 
  published: Advances in Neural Information Processing Systems
  volume: 33
  abstract: >
"Density-ratio estimation via classification is a cornerstone of unsupervised learning. It has provided the foundation for state-of-the-art methods in representation learning and generative modelling, with the number of use-cases continuing to proliferate. However, it suffers from a critical limitation: it fails to accurately estimate ratios p/q for which the two densities differ significantly. Empirically, we find this occurs whenever the KL divergence between p and q exceeds tens of nats. To resolve this limitation, we introduce a new framework, telescoping density-ratio estimation (TRE), that enables the estimation of ratios between highly dissimilar densities in high-dimensional spaces. Our experiments demonstrate that TRE can yield substantial improvements over existing single-ratio methods for mutual information estimation, representation learning and energy-based modelling."

- title: "Variational noise-contrastive estimation"
  authors: Benjamin Rhodes, Michael U Gutmann
  year: 2019 
  month:  4
  type: conference 
  published: The 22nd International Conference on Artificial Intelligence and Statistics
  pages: 2741-2750
  publisher: PMLR
  abstract: >
Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data.

- title: "Statistical applications of contrastive learning"
  authors: Michael U Gutmann, Steven Kleinegesse, Benjamin Rhodes
  year: 2022 
  month:  6
  type: journal 
  published: Behaviormetrika
  pages: 1-25
  publisher: Springer Japan
  abstract: >
The likelihood function plays a crucial role in statistical inference and experimental design. However, it is computationally intractable for several important classes of statistical models, including energy-based models and simulator-based models. Contrastive learning is an intuitive and computationally feasible alternative to likelihood-based learning. We here first provide an introduction to contrastive learning and then show how we can use it to derive methods for diverse statistical problems, namely parameter estimation for energy-based models, Bayesian inference for simulator-based models, as well as experimental design.

<div id="gsc_oci_title">Enhanced gradient-based MCMC in discrete spaces"
  authors: Benjamin Rhodes, Michael Gutmann
  year: 2022/10
  type: journal 
  published: Transactions on Machine Learning Research
  publisher: https://openreview.net/forum?id=j2Mid5hF

- title: "Enhanced gradient-based MCMC in discrete spaces"
  authors: Benjamin Rhodes, Michael Gutmann
  year: 2022 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2208.00040
  abstract: >

- title: "Variational Gibbs inference for statistical model estimation from incomplete data"
  authors: Vaidotas Simkus, Benjamin Rhodes, Michael U Gutmann
  year: 2021 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:2111.13180
  abstract: >

- title: "Scaling Densities For Improved Density Ratio Estimation"
  authors: Akash Srivastava, Seungwook Han, Benjamin Rhodes, Kai Xu, Michael U Gutmann
  year: 2021 
  month:  9
  abstract: >


- title: "Dimension Formulae for Iterated Function Systems"
  authors: Benjamin Rhodes
  abstract: >

<div id="gsc_oci_title">Telescoping Density-Ratio Estimation: Supplementary Material"
  authors: Benjamin Rhodes, Kai Xu, Michael U Gutmann

- title: "Variational Noise Contrastive Estimation&#8212;Supplementary Materials&#8212;"
  authors: Benjamin Rhodes, Michael Gutmann
  abstract: >

- title: "FIMAP: feature importance by minimal adversarial perturbation"
  authors: Matt Chapman-Rounds, Umang Bhatt, Erik Pazos, Marc-Andre Schulz, Konstantinos Georgatzis
  year: 2021 
  month:  5
  type: journal 
  published: Proceedings of the AAAI Conference on Artificial Intelligence
  volume: 35<div class="gsc_oci_field">Issue13
  pages: 11433-11441
  abstract: >
Instance-based model-agnostic feature importance explanations (LIME, SHAP, L2X) are a popular form of algorithmic transparency. These methods generally return either a weighting or subset of input features as an explanation for the classification of an instance. An alternative literature argues instead that counterfactual instances, which alter the black-box model's classification, provide a more actionable form of explanation. We present Feature Importance by Minimal Adversarial Perturbation (FIMAP), a neural network based approach that unifies feature importance and counterfactual explanations. We show that this approach combines the two paradigms, recovering the output of feature-weighting methods in continuous feature spaces, whilst indicating the direction in which the nearest counterfactuals can be found. Our method also provides an implicit confidence estimate in its own explanations, something existing methods lack. Additionally, FIMAP improves upon the speed of sampling-based methods, such as LIME, by an order of magnitude, allowing for explanation deployment in time-critical applications. We extend our approach to categorical features using a partitioned Gumbel layer and demonstrate its efficacy on standard datasets.

- title: "Inferring disease subtypes from clusters in explanation space"
  authors: Marc-Andre Schulz, Matt Chapman-Rounds, Manisha Verma, Danilo Bzdok, Konstantinos Georgatzis
  year: 2020 
  month:  7
  type: journal 
  published: Scientific Reports
  abstract: >

- title: "Comparative Efficacy and Safety of Tirbanibulin for Actinic Keratosis of the Face and Scalp in Europe: A Systematic Review and Network Meta-Analysis of Randomized Controlled Trials"
  authors: Markus V Heppt, Igor Dykukha, Sara Graziadio, Rafael Salido-Vallejo, Matt Chapman-Rounds, Mary Edwards
  year: 2022 
  month:  3<div class="gsc_oci_field">SourceJournal of clinical medicine
  volume: 11<div class="gsc_oci_field">Issue6
  pages: 1654
  publisher: MDPI
  abstract: >
Actinic keratosis (AK) is a chronic skin condition that may progress to cutaneous squamous cell carcinoma. We conducted a systematic review of efficacy and safety for key treatments for AK of the face and scalp, including the novel 5-day tirbanibulin 1% ointment. MEDLINE, PubMed, Embase, Cochrane Library, clinical trial registries and regulatory body websites were searched. The review included 46 studies, of which 35 studies included interventions commonly used in Europe and were sufficiently homogenous to inform a Bayesian network meta-analysis of complete clearance against topical placebo or vehicle. The network meta-analysis revealed the following odds ratios and 95% credible intervals: cryosurgery 13.4 (6.2&#8211;30.3); diclofenac 3% 2.9 (1.9&#8211;4.3); fluorouracil 0.5% + salicylic acid 7.6 (4.6&#8211;13.5); fluorouracil 4% 30.3 (9.1&#8211;144.7); fluorouracil 5% 35.0 (10.2&#8211;164.4); imiquimod 3.75% 8.5 (3.5&#8211;22.4); imiquimod 5% 17.9 (9.1&#8211;36.6); ingenol mebutate 0.015% 12.5 (8.1&#8211;19.9); photodynamic therapy with aminolevulinic acid 24.1 (10.9&#8211;52.8); photodynamic therapy with methyl aminolevulinate 11.7 (6.0&#8211;21.9); tirbanibulin 1% 11.1 (6.2&#8211;20.9). Four sensitivity analyses, from studies assessing efficacy after one treatment cycle only, for &#8804;25 cm<sup>2</sup> treatment area, after 8 weeks post-treatment, and with single placebo/vehicle node confirmed the findings from the base case. Safety outcomes were assessed qualitatively. These results suggest that tirbanibulin 1% offers a novel treatment for AK, with a single short treatment period, favourable safety profile and efficacy, in line with existing topical treatments available in Europe.

- title: "Field Treatments for Actinic Keratosis: A Systematic Review and Network Meta-Analysis"
  authors: G Martin, B Berman, S Feldman, A Armstrong, M Edwards, S Graziadio, R McCool, M Arber, E Carr, D James, M Chapman-Rounds, E Fumero, M Schuchardt, A Grada
  year: 2021 
  month:  11<div class="gsc_oci_field">SourceSKIN The Journal of Cutaneous Medicine
  volume: 5<div class="gsc_oci_field">Issue6
  pages: s81-s81
  abstract: >

- title: "Inattentional Blindness in Visual Search"
  authors: Matt Chapman-Rounds, Christopher G. Lucas, Frank Keller
  year: 2019/7
  type: conference 
  published: Annual Meeting of the Cognitive Science Society<div class="gsc_oci_field">Issue41
  pages: 2688-2694
  abstract: >
Models of visual saliency normally belong to one of two camps: models such as Experience Guided Search (E-GS), which emphasize top-down guidance based on task features, and models such as Attention as Information Maximisation (AIM), which emphasize the role of bottom-up saliency. In this paper, we show that E-GS and AIM are structurally similar and can be unified to create a general model of visual search which includes a generic prior over potential non-task related objects. We demonstrate that this model displays inattentional blindness, and that blindness can be modulated by adjusting the relative precisions of several terms within the model. At the same time, our model correctly accounts for a series of classical visual search results.

- title: "Bootstrapping a Crosslingual Semantic Parser"
  authors: Tom Sherborne, Yumo Xu, Mirella Lapata
  year: 2020 
  month:  11
  type: conference 
  published: Findings of the Association for Computational Linguistics: EMNLP 2020
  pages: 499-517
  abstract: >

- title: "On the Impact of Fixed Point Hardware for Optical Fiber Nonlinearity Compensation Algorithms"
  authors: 
  year: 2018 
  month:  10
  type: journal 
  published: Journal of Lightwave Technology
  volume: 36<div class="gsc_oci_field">Issue20
  pages: 5016-5022
  publisher: IEEE
  abstract: >

- title: "Going beneath the surface: Evaluating image captioning for grammaticality, truthfulness and diversity"
  authors: Huiyuan Xie, Tom Sherborne, Alexander Kuhnle, Ann Copestake
  year: 2020 
  month:  2
  type: conference 
  published: Evaluating Evaluation of AI Systems Workshop at the thirty fourth AAAI Conference on Artificial Intelligence
  publisher: Association for the Advancement of Artificial Intelligence (www.aaai.org)
  abstract: >

- title: "Zero-Shot Cross-lingual Semantic Parsing"
  authors: Tom Sherborne, Mirella Lapata
  year: 2022/5
  type: conference 
  published: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
  volume: 1
  pages: 4134&#8211;4153
  abstract: >

<div id="gsc_oci_title">Meta-Learning a Cross-lingual Manifold for Semantic Parsing"
  authors: Tom Sherborne, Mirella Lapata
  year: 2022 
  month:  9
  type: journal 
  published: Transactions of the ACL

- title: "Robot learning by demonstration with local gaussian process regression"
  authors: Markus Schneider, Wolfgang Ertel
  year: 2010 
  month:  10
  type: conference 
  published: 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems
  pages: 255-260
  publisher: IEEE
  abstract: >

- title: "Expected similarity estimation for large-scale batch and streaming anomaly detection"
  authors: Markus Schneider, Wolfgang Ertel, Fabio Ramos
  year: 2016/12
  type: journal 
  published: Machine Learning
  volume: 105<div class="gsc_oci_field">Issue3
  pages: 305-333
  publisher: Springer US
  abstract: >

- title: "The teaching-box: A universal robot learning framework"
  authors: Wolfgang Ertel, Markus Schneider, Richard Cubek, Michel Tokic
  year: 2009 
  month:  6
  type: conference 
  published: 2009 International Conference on Advanced Robotics
  pages: 1-6
  publisher: IEEE
  abstract: >

- title: "Lat: A simple learning from demonstration method"
  authors: Benjamin Reiner, Wolfgang Ertel, Heiko Posenauer, Markus Schneider
  year: 2014 
  month:  9
  type: conference 
  published: 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems
  pages: 4436-4441
  publisher: IEEE
  abstract: >

- title: "Probability inequalities for kernel embeddings in sampling without replacement"
  authors: Markus Schneider
  year: 2016 
  month:  5
  type: conference 
  published: Artificial Intelligence and Statistics
  pages: 66-74
  publisher: PMLR
  abstract: >
The\emphkernel embedding of distributions is a popular machine learning technique to manipulate probability distributions and an integral part of numerous applications. Its empirical counterpart is an estimate from a finite dataset of samples from the distribution under consideration. However, for large-scale learning problems the empirical kernel embedding becomes infeasible to compute and approximate, constant time, solutions are necessary. Instead of the full dataset, a random subset of smaller size can be used to calculate the empirical kernel embedding, known as\emphsampling without replacement. In this work we generalize the results of (Serfling 1974) to quantify the difference between this two estimates. We derive probability inequalities for the kernel embedding and more general inequalities for Banach space valued martingales in the setting of sampling without replacement.

- title: "Expected similarity estimation for large scale anomaly detection"
  authors: 
  year: 2015 
  month:  7
  type: conference 
  published: 2015 International Joint Conference on Neural Networks (IJCNN)
  pages: 1-8
  publisher: IEEE
  abstract: >

- title: "Learning from demonstration with Gaussian processes"
  authors: Markus Schneider
  year: 2009/9
  institution: Ph. D. dissertation, Master-thesis, University of Applied Sciences Ravensburg-Weingarten, 2009. 1.1, 1.2. 4, B
  abstract: >
In the last years robots turned from simple preprogrammed tools into highly flexible machines which are very complex. Therefore it is very difficult to program such a robot and other human-robot interfaces are needed. This is one reason why Learning from Demonstration (LfD) had become a major topic in the context of robotics. This thesis presents a probabilistic framework for robot skill acquisition using Gaussian processes. The proposed approach is able to extract all necessary informations for a reproduction from a relatively small number of demonstrations and is also capable to observe the common characteristics of the task.

- title: "Kernel feature maps from arbitrary distance metrics"
  authors: 
  year: 2015 
  month:  9
  type: conference 
  published: 
  pages: 137-150
  publisher: Springer, Cham
  abstract: >

- title: "Expected similarity estimation for large-scale anomaly detection"
  authors: Markus Schneider
  year: 2017 
  month:  2
  institution: 
  abstract: >

- title: "Transductive Learning for Multi-Task Copula Processes"
  authors: Markus Schneider, Fabio Ramos
  year: 2014
  type: conference 
  published: European Conference  on Artificial Intelligence (ECAI)
  abstract: >

- title: "Combining gaussian processes and conventional path planning in a learning from demonstration framework"
  authors: Markus Schneider, Richard Cubek, Tobias Fromm, Wolfgang Ertel
  year: 2010 
  month:  5
  type: conference 
  published: International Conference on Research and Education in Robotics
  pages: 118-129
  publisher: Springer, Berlin, Heidelberg
  abstract: >
Today, robots are already able to solve specific tasks in laboratory environments. Since everyday environments are more complex, the robot skills required to solve everyday tasks cannot be known in advance and thus not be programmed beforehand. Rather, the robot must be able to learn those tasks being instructed by users without any technical background. Hence, Learning from Demonstration (LfD) is one of the essential topics to bring robots out of the lab moving towards everyday robustness. The key property of an agent regarding a demonstration learned skill is its ability of generalization, that is, applying a learned skill to situations that differ from those during demonstration. In this paper, we present a method to enhance the generalization capabilities of an advanced new LfD framework by combining it with conventional path planning.

- title: "Reinforcement learning with RBF-networks"
  authors: Markus Schneider
  year: 2006
  type: journal 
  published: Scientific Project, University of Applied Sciences Weingarten
  abstract: >
As far as we leave the domain of small state and action spaces, we cannot represent the value function as a table anymore. Generalization is needed. Radial Basis Functions are often used in reinforcement learning as linear function approximation to learn a value function. These functions have very good convergence guarantees and producing very smooth approximations.

- title: "Constant Time EXPected Similarity Estimation for Large-Scale Anomaly Detection."
  authors: 
  year: 2016 
  month:  8
  type: conference 
  published: ECAI
  pages: 12-20
  abstract: >
A new algorithm named EXPected Similarity Estimation (EXPOSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of n samples, EXPOSE takes O (n) time to build a model and O (1) time per prediction. In this work we describe and analyze a simple and effective stochastic optimization algorithm which allows us to drastically reduce the learning time of EXPOSE from previous linear to constant. It is crucial that this approach allows us to determine the number of iterations based on a desired accuracy, independent of the dataset size n. We will show that the proposed stochastic gradient descent algorithm works in general possible infinite-dimensional Hilbert spaces, is easy to implement and requires no additional step-size parameters.

"
  authors: Markus Schneider
  year: 2007/7
  institution: Verlag nicht ermittelbar
  abstract: >

<div id="gsc_oci_title">Kernel Embeddings for Large-Scale Anomaly Detection"
  authors: Markus Schneider, Wolfgang Ertel, Fabio Ramos
  year: 2016
  type: journal 
  published: International Conference on Machine Learning (ICML 2016): Anomaly Detection Workshop

- title: "Constant Time EXPected Similarity Estimation using Stochastic Optimization"
  authors: 
  year: 2015 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1511.05371
  abstract: >

- title: "Tucker: Tensor factorization for knowledge graph completion"
  authors: Ivana Bala&#382;evi&#263;, Carl Allen, Timothy M Hospedales
  year: 2019 
  month:  1
  type: journal 
  published: arXiv preprint arXiv:1901.09590
  abstract: >

- title: "Multi-scale attributed node embedding"
  authors: Benedek Rozemberczki, Carl Allen, Rik Sarkar
  year: 2021/4
  type: journal 
  published: Journal of Complex Networks
  volume: 9<div class="gsc_oci_field">Issue2
  pages: cnab014
  publisher: Oxford University Press
  abstract: >
We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.

"
  authors: Ivana Balazevic, Carl Allen, Timothy Hospedales
  year: 2019
  type: journal 
  published: Advances in Neural Information Processing Systems
  volume: 32
  abstract: >

- title: "Hypernetwork knowledge graph embeddings"
  authors: Ivana Bala&#382;evi&#263;, Carl Allen, Timothy M Hospedales
  year: 2019 
  month:  9
  type: conference 
  published: International Conference on Artificial Neural Networks
  pages: 553-565
  publisher: Springer, Cham
  abstract: >

- title: "Biomimetic synthesis of lantibiotics"
  authors: Sarah Burrage, Tony Raynham, Glyn Williams, Jonathan W Essex, Carl Allen, Marianne Cardno, Vinay Swali, Mark Bradley
  year: 2000 
  month:  4
  type: journal 
  published: Chemistry&#8211;A European Journal
  volume: 6<div class="gsc_oci_field">Issue8
  pages: 1455-1466
  publisher: WILEY&#8208;VCH Verlag
  abstract: >
The lantibiotics are a class of highly posttranslationally modified small peptide antibiotics containing numerous lanthionine and dehydroamino acid residues. We have prepared peptides containing multiple dehydroamino acids and cysteine residues in order to probe the biomimetic synthesis of the lantibiotics from their precursor peptides. A novel synthetic methodology was developed to allow the synthesis of multiple dehydroamino acid containing peptides. Cyclisations were rapid, quantitative and regiospecific. Remarkably the peptide sequences alone appear to contain sufficient information to direct a series of stereo&#8208; and regiospecific ring closures. Thus both the two linear peptides for the B and E&#8208;rings closed stereoselectively. In the case of the A&#8208;ring precursor peptide which contained two dehydroamino acids, cyclisation was again totally regioselective, although not totally stereoselective.

- title: "Analogies Explained: Towards Understanding Word Embeddings"
  authors: Carl Allen, Tim Hospedales
  year: 2019 
  month:  6
  type: conference 
  published: International Conference on Machine Learning
  volume: 97
  pages: 223-231
  abstract: >
Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, eg the embeddings of analogy &#8220;woman is to queen as man is to king&#8221; approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of &#8220;<svg class="gs_fsvg" aria-label=" w_x " width="17px" height="8px" style="vertical-align:-2px;"><g transform="matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.20200)"><g><path transform="scale(0.48828, -0.48828)" d="M 221 238  Q 221 281 232 337  T 258 436  T 305 570  T 342 668  Q 369 743 369 791  Q 369 852 324 852  Q 243 852 190 768  T 113 582  Q 109 569 96 569  H 72  Q 55 569 55 588  V 594  Q 88 716 156 810  T 328 905  Q 401 905 451 857  T 502 735  Q 502 697 485 655  Q 461 593 428 506  T 378 348  T 360 211  Q 360 131 402 81  T 524 31  Q 633 31 705 197  Q 702 209 702 231  Q 702 286 719 356  L 834 817  Q 841 844 866 863  T 920 883  Q 944 883 962 867  T 981 825  Q 981 813 979 809  L 864 352  Q 844 270 844 205  Q 844 128 879 79  T 991 31  Q 1123 31 1208 205  Q 1240 267 1279 388  T 1319 573  Q 1319 632 1300 666  T 1247 741  T 1214 801  Q 1214 840 1247 873  T 1321 907  Q 1370 907 1391 862  T 1413 760  Q 1413 691 1390 584  T 1332 370  T 1266 199  Q 1154 -23 987 -23  Q 901 -23 830 10  T 725 115  Q 692 56 638 16  T 520 -23  Q 385 -23 303 41  T 221 238  Z "/><g transform="translate(715.94000, 150.00000)"><path transform="scale(0.34180, -0.34180)" d="M 215 76  Q 257 45 324 45  Q 390 45 441 102  T 508 227  L 600 596  Q 621 674 621 713  Q 621 769 588 803  T 500 838  Q 433 838 375 801  T 275 705  T 217 582  Q 214 563 197 563  H 164  Q 155 563 148 572  T 141 588  V 596  Q 174 721 277 812  T 504 903  Q 554 903 602 887  T 689 841  T 750 766  Q 791 826 850 864  T 973 903  Q 1058 903 1125 863  T 1192 745  Q 1192 710 1176 679  T 1132 630  T 1069 612  Q 1035 612 1011 633  T 987 690  Q 987 731 1013 764  T 1079 807  Q 1034 838 969 838  Q 924 838 886 810  T 822 743  T 782 655  L 690 287  Q 672 204 672 170  Q 672 114 704 79  T 793 45  Q 844 45 890 67  T 975 127  T 1039 211  T 1075 301  Q 1082 319 1098 319  H 1130  Q 1140 319 1146 311  T 1153 295  Q 1153 291 1151 287  Q 1131 205 1077 135  T 947 22  T 788 -20  Q 710 -20 643 15  T 543 117  Q 505 59 445 19  T 319 -20  Q 237 -20 169 19  T 102 137  Q 102 190 137 230  T 225 270  Q 260 270 283 249  T 307 193  Q 307 152 281 118  T 215 76  Z "/></g></g></g></svg> is to <svg class="gs_fsvg" aria-label=" w_y " width="16px" height="10px" style="vertical-align:-4px;"><g transform="matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.20200)"><g><path transform="scale(0.48828, -0.48828)" d="M 221 238  Q 221 281 232 337  T 258 436  T 305 570  T 342 668  Q 369 743 369 791  Q 369 852 324 852  Q 243 852 190 768  T 113 582  Q 109 569 96 569  H 72  Q 55 569 55 588  V 594  Q 88 716 156 810  T 328 905  Q 401 905 451 857  T 502 735  Q 502 697 485 655  Q 461 593 428 506  T 378 348  T 360 211  Q 360 131 402 81  T 524 31  Q 633 31 705 197  Q 702 209 702 231  Q 702 286 719 356  L 834 817  Q 841 844 866 863  T 920 883  Q 944 883 962 867  T 981 825  Q 981 813 979 809  L 864 352  Q 844 270 844 205  Q 844 128 879 79  T 991 31  Q 1123 31 1208 205  Q 1240 267 1279 388  T 1319 573  Q 1319 632 1300 666  T 1247 741  T 1214 801  Q 1214 840 1247 873  T 1321 907  Q 1370 907 1391 862  T 1413 760  Q 1413 691 1390 584  T 1332 370  T 1266 199  Q 1154 -23 987 -23  Q 901 -23 830 10  T 725 115  Q 692 56 638 16  T 520 -23  Q 385 -23 303 41  T 221 238  Z "/><g transform="translate(715.94000, 150.00000)"><path transform="scale(0.34180, -0.34180)" d="M 176 -238  Q 176 -182 210 -141  T 299 -100  Q 334 -100 358 -122  T 383 -178  Q 383 -223 352 -257  T 276 -297  Q 323 -352 424 -352  Q 567 -352 682 -201  Q 724 -145 750 -82  T 797 68  Q 699 -20 582 -20  Q 451 -20 370 45  T 289 240  Q 289 331 320 424  T 414 670  Q 436 729 436 770  Q 436 799 424 818  T 387 838  Q 305 838 249 758  T 168 582  Q 165 563 147 563  H 115  Q 106 563 99 572  T 92 588  V 596  Q 112 672 153 741  T 255 857  T 391 903  Q 444 903 489 882  T 563 821  T 592 729  Q 592 695 578 655  Q 515 500 482 400  T 449 209  Q 449 138 482 91  T 586 45  Q 727 45 834 219  L 981 807  Q 990 840 1017 861  T 1077 883  Q 1107 883 1128 864  T 1149 817  Q 1149 810 1148 806  T 1147 797  L 946 -8  Q 918 -120 837 -215  T 646 -364  T 420 -418  Q 327 -418 251 -372  T 176 -238  Z "/></g></g></g></svg>&#8221;. From these concepts we prove existence of linear relationship between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.

- title: "Interpreting knowledge graph relation representation from word embeddings"
  authors: Carl Allen, Ivana Bala&#382;evi&#263;, Timothy Hospedales
  year: 2019 
  month:  9
  type: journal 
  published: arXiv preprint arXiv:1909.11611
  abstract: >

- title: "What the Vec? Towards Probabilistically Grounded Embeddings"
  authors: Carl Allen, Ivana Bala&#382;evi&#263;, Timothy Hospedales
  year: 2019 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:1805.12164
  abstract: >
Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast, efficient and their embeddings widely used. Moreover, the W2V algorithm has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and the relative simplicity of their common architecture, what the embedding parameters of W2V and Glove learn, and why that it useful in downstream tasks largely remains a mystery. We show that different interactions of PMI vectors encode semantic properties that can be captured in low dimensional word embeddings by suitable projection, theoretically explaining why the embeddings of W2V and Glove work, and, in turn, revealing an interesting mathematical interconnection between the semantic relationships of relatedness, similarity, paraphrase and analogy.

- title: "Benchmark and best practices for biomedical knowledge graph embeddings"
  authors: David Chang, Ivana Bala&#382;evi&#263;, Carl Allen, Daniel Chawla, Cynthia Brandt, Richard Andrew Taylor
  year: 2020/7
  type: journal 
  published: Proceedings of the conference. Association for Computational Linguistics. Meeting
  volume: 2020
  pages: 167
  publisher: NIH Public Access
  abstract: >

- title: "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning"
  authors: Carl Allen, Ivana Bala&#382;evi&#263;, Timothy Hospedales
  year: 2020 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2006.05896
  abstract: >

- title: "Adapters for Enhanced Modeling of Multilingual Knowledge and Text"
  authors: Yifan Hou, Wenxiang Jiao, Meizhen Liu, Zhaopeng Tu, Carl Allen, Mrinmaya Sachan
  year: 2022 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2210.13617
  abstract: >

- title: "Learning to Drop Out: An Adversarial Approach to Training Sequence VAEs"
  authors: &#272;or&#273;e Miladinovi&#263;, Kumar Shridhar, Kushal Jain, Max B Paulus, Joachim M Buhmann, Carl Allen
  year: 2022 
  month:  9
  type: journal 
  published: arXiv preprint arXiv:2209.12590
  abstract: >

- title: "Towards a theoretical understanding of word and relation representation"
  authors: Carl Allen
  year: 2022 
  month:  2
  type: journal 
  published: arXiv preprint arXiv:2202.00486
  abstract: >

- title: "Learning the Prediction Distribution for Semi-Supervised Learning with Normalising Flows"
  authors: Ivana Bala&#382;evi&#263;, Carl Allen, Timothy Hospedales
  year: 2020 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2007.02745
  abstract: >

- title: "Data Augmentation Generative Adversarial Networks"
  authors: Antreas Antoniou, Amos Storkey, Harrison Edwards
  year: 2017 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1711.04340
  abstract: >

- title: "Meta-learning in neural networks: A survey"
  authors: Timothy Hospedales, Antreas Antoniou, Paul Micaelli, Amos Storkey
  year: 2021 
  month:  5
  type: journal 
  published: IEEE transactions on pattern analysis and machine intelligence
  volume: 44<div class="gsc_oci_field">Issue9
  pages: 5149-5169
  publisher: IEEE
  abstract: >

- title: "How to train your MAML"
  authors: Antreas Antoniou, Harrison Edwards, Amos Storkey
  year: 2018 
  month:  10
  type: conference 
  published: International Conference on Learning Representations 2019
  abstract: >

- title: "CINIC-10 is not ImageNet or CIFAR-10"
  authors: Luke N Darlow, Elliot J Crowley, Antreas Antoniou, Amos J Storkey
  year: 2018 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1810.03505
  abstract: >

- title: "Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation"
  authors: Antreas Antoniou, Amos Storkey
  year: 2019 
  month:  2
  type: journal 
  published: arXiv preprint arXiv:1902.09884
  abstract: >

- title: "Learning to Learn by Self-Critique"
  authors: Antreas Antoniou, Amos J Storkey
  year: 2019
  type: conference 
  published: Advances in Neural Information Processing Systems 2019
  pages: 9936-9946
  abstract: >
In few-shot learning, a machine learning system is required to learn from a small set of labelled examples of a specific task, such that it can achieve strong generalization on new unlabelled examples of the same task. Given the limited availability of labelled examples in such tasks, we need to make use of all the information we can. For this reason we propose the use of transductive meta-learning for few shot settings to obtain state-of-the-art few-shot learning.

- title: "Defining benchmarks for continual few-shot learning"
  authors: Antreas Antoniou, Massimiliano Patacchiola, Mateusz Ochal, Amos Storkey
  year: 2020 
  month:  4
  type: journal 
  published: arXiv preprint arXiv:2004.11967
  abstract: >

- title: "A general purpose intelligent surveillance system for mobile devices using deep learning"
  authors: Antreas Antoniou, Plamen Angelov
  year: 2016 
  month:  7
  type: conference 
  published: 2016 International Joint Conference on Neural Networks (IJCNN)
  pages: 2879-2886
  publisher: IEEE
  abstract: >

- title: "Dilated Densenets for Relational Reasoning"
  authors: Antreas Antoniou, Agnieszka S&#322;owik, Elliot J Crowley, Amos Storkey
  year: 2018 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1811.00410
  abstract: >

- title: "Meta-meta-learning for Neural Architecture Search through arXiv Descent"
  authors: Antreas Antoniou, Nick Pawlowski, Jack Turner, James Owers, Joseph Mellor, Elliot J Crowley
  abstract: >
Recent work in meta-learning has set the deep learning community alight. From minute gains on few-shot learning tasks, to discovering architectures that are slightly better than chance, to solving intelligence itself 1, meta-learning is proving a popular solution to every conceivable problem ever conceivably conceived ever. In this paper we venture deeper into the computational insanity that is meta-learning, and potentially risk exiting the simulation of reality itself, by attempting to metalearn at a third learning level. We showcase the resulting approach&#8212;which we call meta-meta-learning&#8212;for neural architecture search. Crucially, instead of meta-learning a neural architecture differentiably as in DARTS (Liu et al., 2018) we meta-meta-learn an architecture by searching through arXiv. This arXiv descent is GPU-free and only requires a handful of graduate students. Further, we introduce a regulariser, called college-dropout, which works by randomly removing a single graduate student from our system. As a consequence, procrastination levels decrease significantly, due to the increased workload and sense of responsibility each student attains. The code for our experiments is publicly available at. Edit: we have decided not to release our code as we are concerned that it may be used for malicious purposes.

- title: "Meta learning for supervised and unsupervised few-shot learning"
  authors: Antreas Antoniou
  year: 2021 
  month:  11
  publisher: The University of Edinburgh
  abstract: >

- title: "Tucker: Tensor factorization for knowledge graph completion"
  authors: Ivana Bala&#382;evi&#263;, Carl Allen, Timothy M Hospedales
  year: 2019 
  month:  1
  type: journal 
  published: arXiv preprint arXiv:1901.09590
  abstract: >

"
  authors: Ivana Balazevic, Carl Allen, Timothy Hospedales
  year: 2019
  type: journal 
  published: Advances in Neural Information Processing Systems
  volume: 32
  abstract: >

- title: "Hypernetwork Knowledge Graph Embeddings"
  authors: Ivana Bala&#382;evi&#263;, Carl Allen, Timothy M Hospedales
  year: 2019 
  month:  9
  type: conference 
  published: International Conference on Artificial Neural Networks
  pages: 553-565
  publisher: Springer, Cham
  abstract: >

- title: "Cutting down on prompts and parameters: Simple few-shot learning with language models"
  authors: Robert L Logan IV, Ivana Bala&#382;evi&#263;, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel
  year: 2021 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2106.13353
  abstract: >

- title: "Interpreting Knowledge Graph Relation Representation from Word Embeddings"
  authors: Carl Allen, Ivana Bala&#382;evic, Timothy Hospedales
  year: 2021
  type: journal 
  published: International Conference on Learning Representations
  abstract: >

- title: "What the Vec? Towards Probabilistically Grounded Embeddings"
  authors: Carl Allen, Ivana Balazevic, Timothy Hospedales
  year: 2019
  type: conference 
  published: Advances in Neural Information Processing Systems
  pages: 7465-7475
  abstract: >
Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast, efficient and their embeddings widely used. Moreover, the W2V algorithm has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and the relative simplicity of their common architecture, what the embedding parameters of W2V and Glove learn, and why that it useful in downstream tasks largely remains a mystery. We show that different interactions of PMI vectors encode semantic properties that can be captured in low dimensional word embeddings by suitable projection, theoretically explaining why the embeddings of W2V and Glove work, and, in turn, revealing an interesting mathematical interconnection between the semantic relationships of relatedness, similarity, paraphrase and analogy.

- title: "Benchmark and best practices for biomedical knowledge graph embeddings"
  authors: David Chang, Ivana Bala&#382;evi&#263;, Carl Allen, Daniel Chawla, Cynthia Brandt, Richard Andrew Taylor
  year: 2020/7
  type: journal 
  published: Proceedings of the conference. Association for Computational Linguistics. Meeting
  volume: 2020
  pages: 167
  publisher: NIH Public Access
  abstract: >

- title: "Language detection for short text messages in social media"
  authors: 
  year: 2016 
  month:  8
  type: journal 
  published: arXiv preprint arXiv:1608.08515
  abstract: >

- title: "A Probabilistic Model for Discriminative and Neuro-Symbolic Semi-Supervised Learning"
  authors: Carl Allen, Ivana Bala&#382;evi&#263;, Timothy Hospedales
  year: 2020 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2006.05896
  abstract: >

- title: "Learning representations of entities and relations"
  authors: Ivana Bala&#382;evi&#263;
  year: 2022 
  month:  1
  type: journal 
  published: arXiv preprint arXiv:2201.13073
  abstract: >

- title: "Learning the Prediction Distribution for Semi-Supervised Learning with Normalising Flows"
  authors: Ivana Bala&#382;evi&#263;, Carl Allen, Timothy Hospedales
  year: 2020 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2007.02745
  abstract: >

- title: "Neural spline flows"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2019
  type: conference 
  published: Advances in Neural Information Processing Systems
  pages: 7511-7522
  abstract: >
A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.

- title: "Cubic-Spline Flows"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2019 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1906.02145
  abstract: >

- title: "nflows: normalizing flows in PyTorch"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2020<div class="gsc_oci_field">Sourcehttps://doi.org/10.5281/zenodo.4296286
  publisher: Zenodo

- title: "Bayesian Adversarial Spheres: Bayesian Inference and Adversarial Examples in a Noiseless Setting"
  authors: Artur Bekasov, Iain Murray
  year: 2018 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1811.12335
  abstract: >

- title: "Ordering Dimensions with Nested Dropout Normalizing Flows"
  authors: Artur Bekasov, Iain Murray
  year: 2020 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2006.08777
  abstract: >

- title: "Accurate and reliable probabilistic modeling with high-dimensional data"
  authors: Artur Bekasov
  year: 2022 
  month:  6
  publisher: The University of Edinburgh
  abstract: >

- title: "Cinic-10 is not imagenet or cifar-10"
  authors: Luke N Darlow, Elliot J Crowley, Antreas Antoniou, Amos J Storkey
  year: 2018 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1810.03505
  abstract: >

- title: "Fingerprint minutiae extraction using deep learning"
  authors: Luke Nicholas Darlow, Benjamin Rosman
  year: 2017 
  month:  10
  type: conference 
  published: 2017 IEEE International Joint Conference on Biometrics (IJCB)
  pages: 22-30
  publisher: IEEE
  abstract: >

- title: "Efficient internal and surface fingerprint extraction and blending using optical coherence tomography"
  authors: Luke Nicholas Darlow, James Connan
  year: 2015 
  month:  11
  type: journal 
  published: Applied optics
  volume: 54<div class="gsc_oci_field">Issue31
  pages: 9258-9268
  publisher: Optica Publishing Group
  abstract: >

- title: "Internal fingerprint zone detection in optical coherence tomography fingertip scans"
  authors: Luke Nicholas Darlow, James Connan, Sharat Saurabh Akhoury
  year: 2015/4
  type: journal 
  published: Journal of Electronic Imaging
  volume: 24<div class="gsc_oci_field">Issue2
  pages: 023027
  publisher: SPIE
  abstract: >

- title: "Automated spoof-detection for fingerprints using optical coherence tomography"
  authors: Luke Nicholas Darlow, Leandra Webb, Natasha Botha
  year: 2016 
  month:  5
  type: journal 
  published: Applied optics
  volume: 55<div class="gsc_oci_field">Issue13
  pages: 3387-3396
  publisher: Optical Society of America
  abstract: >
Fingerprint recognition systems are prevalent in high-security applications. As a result, the act of spoofing these systems with artificial fingerprints is of increasing concern. This research presents an automatic means for spoof-detection using optical coherence tomography (OCT). This technology is able to capture a 3D representation of the internal structure of the skin and is thus not limited to a 2D surface scan. The additional information afforded by this representation means that accurate spoof-detection can be achieved. Two features were extracted to detect the presence of (1) an additional thin layer on the surface of the skin and (2) a thicker additional layer or a complete artificial finger. An analysis of these features showed that they are highly separable, resulting in 100% accuracy regarding spoof-detection, with no false rejections of real fingers. This is the first attempt at fully automated spoof-detection using OCT.

- title: "A review of state-of-the-art speckle reduction techniques for optical coherence tomography fingertip scans"
  authors: Luke Nicholas Darlow, Sharat Saurabh Akhoury, James Connan
  year: 2015 
  month:  2<div class="gsc_oci_field">SourceSeventh International Conference on Machine Vision (ICMV 2014)
  volume: 9445
  pages: 418-426
  publisher: SPIE
  abstract: >

- title: "Extracting subsurface fingerprints using optical coherence tomography"
  authors: Sharat Saurabh Akhoury, Luke Nicholas Darlow
  year: 2015 
  month:  2
  type: conference 
  published: 2015 Third International Conference on Digital Information, Networking, and Wireless Communications (DINWC)
  pages: 184-187
  publisher: IEEE
  abstract: >

- title: "Study on internal to surface fingerprint correlation using optical coherence tomography and internal fingerprint extraction"
  authors: Luke Nicholas Darlow, James Connan
  year: 2015/12
  type: journal 
  published: Journal of Electronic Imaging
  volume: 24<div class="gsc_oci_field">Issue6
  pages: 063014
  publisher: SPIE
  abstract: >

- title: "Internal fingerprint acquisition from optical coherence tomography fingertip scans"
  authors: Luke Nicholas Darlow, Sharat Saurabh Akhoury, James Connan
  year: 2015 
  month:  2
  type: conference 
  published: 2015 Third International Conference on Digital Information, Networking, and Wireless Communications (DINWC)
  pages: 188-191
  publisher: IEEE
  abstract: >

- title: "Optical coherence tomography for fingerprint presentation attack detection"
  authors: Yaseen Moolla, Luke Darlow, Ameeth Sharma, Ann Singh, Johan Van Der Merwe
  year: 2019
  type: conference 
  published: Handbook of Biometric Anti-Spoofing
  pages: 49-70
  publisher: Springer, Cham
  abstract: >

- title: "Performance analysis of a hybrid fingerprint extracted from optical coherence tomography fingertip scans"
  authors: Luke Nicholas Darlow, James Connan, Ann Singh
  year: 2016 
  month:  6
  type: conference 
  published: 2016 International Conference on Biometrics (ICB)
  pages: 1-8
  publisher: IEEE
  abstract: >

- title: "What information does a ResNet compress?"
  authors: Luke Nicholas Darlow, Amos Storkey
  year: 2020 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:2003.06254
  abstract: >

- title: "Latent adversarial debiasing: Mitigating collider bias in deep neural networks"
  authors: Luke Darlow, Stanis&#322;aw Jastrz&#281;bski, Amos Storkey
  year: 2020 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:2011.11486
  abstract: >

- title: "Dhog: Deep hierarchical object grouping"
  authors: Luke Nicholas Darlow, Amos Storkey
  year: 2020 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:2003.08821
  abstract: >

- title: "Cinic-10 is not imagenet or cifar-10 (2018)"
  authors: LN Darlow, EJ Crowley, A Antoniou, AJ Storkey
  year: 1810
  type: journal 
  published: arXiv preprint arXiv:1810.03505

- title: "Damage invariant and high security acquisition of the internal fingerprint using optical coherence tomography"
  authors: Luke N Darlow, Ann Singh, Yaseen Moolla, Lesiba R Ramokolo, R Van Wyk, Natasha Botha, L Webb-Ray
  year: 2016
  publisher: Infonomics Society
  abstract: >

- title: "BlockSwap: Fisher-guided Block Substitution for Network Compression"
  authors: Jack Turner, Elliot J Crowley, Gavin Gray, Amos J Storkey, Michael FP O&#39;Boyle
  year: 2019 
  month:  1
  abstract: >

- title: "Matching Fingerprints with a Toroidal Iterative Closest Point Algorithm"
  authors: Courtney R Pitcher, Patrick C Marais, Luke N Darlow
  year: 2020 
  month:  9
  type: conference 
  published: Conference of the South African Institute of Computer Scientists and Information Technologists 2020
  pages: 51-57
  abstract: >

- title: "GINN: Geometric Illustration of Neural Networks"
  authors: Luke N Darlow, Amos J Storkey
  year: 2018 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1810.01860
  abstract: >

- title: "Internal fingerprint extraction"
  authors: Luke Nicholas Darlow
  year: 2016/2
  publisher: Rhodes University; Faculty of Science, Computer Science
  abstract: >
Fingerprints are a non-invasive biometric that possess significant advantages. However, they are subject to surface erosion and damage; distortion upon scanning; and are vulnerable to fingerprint spoofing. The internal fingerprint exists as the undulations of the papillary junction-an intermediary layer of skin-and provides a solution to these disadvantages. Optical coherence tomography is used to capture the internal fingerprint.

- title: "Data Augmentation Generative Adversarial Networks"
  authors: Antreas Antoniou, Amos Storkey, Harrison Edwards
  year: 2017 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1711.04340
  abstract: >

- title: "Exploration by Random Network Distillation"
  authors: Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov
  year: 2018 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1810.12894
  abstract: >

- title: "Large-Scale Study of Curiosity-Driven Learning"
  authors: Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, Alexei A Efros
  year: 2018 
  month:  8
  type: journal 
  published: arXiv preprint arXiv:1808.04355
  abstract: >

- title: "How to train your MAML"
  authors: Antreas Antoniou, Harrison Edwards, Amos Storkey
  year: 2018 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1810.09502
  abstract: >

- title: "Censoring representations with an adversary"
  authors: Harrison Edwards, Amos Storkey
  year: 2015 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1511.05897
  abstract: >

- title: "Towards a Neural Statistician"
  authors: Harrison Edwards, Amos Storkey
  year: 2016 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1606.02185
  abstract: >

- title: "Evaluating Large Language Models Trained on Code"
  authors: Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, Will Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba
  year: 2021 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:2107.03374
  abstract: >

- title: "Variational Option Discovery Algorithms"
  authors: Joshua Achiam, Harrison Edwards, Dario Amodei, Pieter Abbeel
  year: 2018 
  month:  7
  type: journal 
  published: arXiv preprint arXiv:1807.10299
  abstract: >

- title: "Learning Policy Representations in Multiagent Systems"
  authors: Aditya Grover, Maruan Al-Shedivat, Jayesh K Gupta, Yura Burda, Harrison Edwards
  year: 2018 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1806.06464
  abstract: >
Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.

- title: "Augmenting Image Classifiers Using Data Augmentation Generative Adversarial Networks"
  authors: Antreas Antoniou, Amos Storkey, Harrison Edwards
  year: 2018 
  month:  10
  type: conference 
  published: International Conference on Artificial Neural Networks
  pages: 594-603
  publisher: Springer, Cham
  abstract: >
Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively, but standard data augmentation produces only limited plausible alternative data. Given the potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, uses data from a source domain and learns to take a data item and augment it by generating other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes. We demonstrate that a Data Augmentation Generative Adversarial Network (DAGAN) augments classifiers well on Omniglot, EMNIST and VGG-Face.

<div id="gsc_oci_title">Grokking: Generalization beyond overfitting on small algorithmic datasets"
  authors: Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra

- title: "Unsupervised Neural Machine Translation with Generative Language Models Only"
  authors: Jesse Michael Han, Igor Babuschkin, Harrison Edwards, Arvind Neelakantan, Tao Xu, Stanislas Polu, Alex Ray, Pranav Shyam, Aditya Ramesh, Alec Radford, Ilya Sutskever
  year: 2021 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2110.05448
  abstract: >

- title: "Evaluating Generalization in Multiagent Systems using Agent-Interaction Graphs"
  authors: Aditya Grover, Maruan Al-Shedivat, Jayesh K Gupta, Yuri Burda, Harrison Edwards
  year: 2018
  type: journal 
  published: International Conference on Autonomous Agents and Multiagent Systems
  abstract: >
Learning from interactions between agents is a key component for inference in multiagent systems. Depending on the downstream task, there could be multiple criteria for evaluating the generalization performance of learning. In this work, we propose a novel framework for evaluating generalization in multiagent systems based on agent-interaction graphs. An agent-interaction graph models agents as nodes and interactions as hyper-edges between participating agents. Using this abstract data structure, we define three notions of generalization for principled evaluation of learning in multiagent systems.

- title: "Average distances on self-similar sets and higher order average distances of self-similar measures"
  authors: D Allen, H Edwards, S Harper, L Olsen
  year: 2017 
  month:  10
  type: journal 
  published: Mathematische Zeitschrift
  volume: 287<div class="gsc_oci_field">Issue1-2
  pages: 287-324
  publisher: Springer Berlin Heidelberg
  abstract: >
The purpose of this paper is twofold: (1) we study different notions of the average distance between two points of a self-similar subset of <svg class="gs_fsvg" aria-label="R" width="10px" height="10px" style="vertical-align:0px;"><g transform="matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 9.56200)"><g><path transform="scale(0.48828, -0.48828)" d="M 66 0  Q 53 0 43 12  T 33 37  Q 33 65 66 72  H 72  Q 139 72 165 77  T 203 108  T 215 193  V 1214  Q 215 1271 202 1295  T 161 1325  T 66 1331  Q 52 1331 42 1343  T 33 1368  Q 33 1396 66 1403  H 743  Q 870 1403 996 1364  T 1205 1240  T 1288 1022  Q 1288 913 1243 840  T 1118 726  T 928 666  Q 1008 541 1088 420  T 1263 186  T 1407 72  Q 1420 72 1430 61  T 1440 37  Q 1440 7 1407 0  H 1053  Q 1042 0 1028 14  L 623 643  H 549  V 188  Q 549 132 562 107  T 603 77  T 698 72  Q 712 72 721 61  T 731 37  Q 731 7 698 0  H 66  Z M 268 72  H 496  Q 477 113 477 188  V 1253  Q 477 1296 504 1331  H 268  Q 287 1290 287 1214  V 188  Q 287 113 268 72  Z M 1075 72  H 1274  Q 1204 140 1133 233  T 1011 405  T 850 655  Q 762 647 707 645  L 1075 72  Z M 549 715  Q 675 715 739 722  T 847 757  T 914 847  T 936 1022  Q 936 1184 878 1257  T 666 1331  Q 621 1331 585 1310  T 549 1249  V 715  Z M 942 741  Q 1028 759 1088 791  T 1183 879  T 1217 1022  Q 1217 1139 1134 1210  T 928 1311  Q 1008 1218 1008 1022  Q 1008 925 997 859  T 942 741  Z "/></g></g></svg>, and (2) we investigate the asymptotic behaviour of higher order average moments of self-similar measures on self-similar subsets of <svg class="gs_fsvg" aria-label="R" width="10px" height="10px" style="vertical-align:0px;"><g transform="matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 9.56200)"><g><path transform="scale(0.48828, -0.48828)" d="M 66 0  Q 53 0 43 12  T 33 37  Q 33 65 66 72  H 72  Q 139 72 165 77  T 203 108  T 215 193  V 1214  Q 215 1271 202 1295  T 161 1325  T 66 1331  Q 52 1331 42 1343  T 33 1368  Q 33 1396 66 1403  H 743  Q 870 1403 996 1364  T 1205 1240  T 1288 1022  Q 1288 913 1243 840  T 1118 726  T 928 666  Q 1008 541 1088 420  T 1263 186  T 1407 72  Q 1420 72 1430 61  T 1440 37  Q 1440 7 1407 0  H 1053  Q 1042 0 1028 14  L 623 643  H 549  V 188  Q 549 132 562 107  T 603 77  T 698 72  Q 712 72 721 61  T 731 37  Q 731 7 698 0  H 66  Z M 268 72  H 496  Q 477 113 477 188  V 1253  Q 477 1296 504 1331  H 268  Q 287 1290 287 1214  V 188  Q 287 113 268 72  Z M 1075 72  H 1274  Q 1204 140 1133 233  T 1011 405  T 850 655  Q 762 647 707 645  L 1075 72  Z M 549 715  Q 675 715 739 722  T 847 757  T 914 847  T 936 1022  Q 936 1184 878 1257  T 666 1331  Q 621 1331 585 1310  T 549 1249  V 715  Z M 942 741  Q 1028 759 1088 791  T 1183 879  T 1217 1022  Q 1217 1139 1134 1210  T 928 1311  Q 1008 1218 1008 1022  Q 1008 925 997 859  T 942 741  Z "/></g></g></svg>.

- title: "The Context-Aware Learner"
  authors: Conor Durkan, Amos Storkey, Harrison Edwards
  year: 2018 
  month:  2
  abstract: >
One important aspect of generalization in machine learning involves reasoning about previously seen data in new settings. Such reasoning requires learning disentangled representations of data which are interpretable in isolation, but can also be combined in a new, unseen scenario. To this end, we introduce the context-aware learner, a model based on the variational autoencoding framework, which can learn such representations across data sets exhibiting a number of distinct contexts. Moreover, it is successfully able to combine these representations to generate data not seen at training time. The model enjoys an exponential increase in representational ability for a linear increase in context count. We demonstrate that the theory readily extends to a meta-learning setting such as this, and describe a fully unsupervised model in complete generality. Finally, we validate our approach using an adaptation with weak supervision.Data:[CelebA](https://paperswithcode. com/dataset/celeba),[MNIST](https://paperswithcode. com/dataset/mnist)4 RepliesLoading

- title: "No-where averagely differentiable functions: Baire category and the Takagi function"
  authors: 
  year: 2013 
  month:  10
  type: journal 
  published: 
  volume: 172<div class="gsc_oci_field">Issue1
  pages: 1-27
  publisher: Springer Vienna
  abstract: >

- title: "Recurrent neural networks as weighted language recognizers"
  authors: Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, Kevin Knight
  year: 2017 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1711.05408
  abstract: >

- title: "Parsing graphs with regular graph grammars"
  authors: Sorcha Gilroy, Adam Lopez, Sebastian Maneth
  year: 2017/8
  type: conference 
  published: Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017)
  pages: 199-208
  abstract: >
Recently, several datasets have become available which represent natural language phenomena as graphs. Hyperedge Replacement Languages (HRL) have been the focus of much attention as a formalism to represent the graphs in these datasets. Chiang et al.(2013) prove that HRL graphs can be parsed in polynomial time with respect to the size of the input graph. We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages (RGL; Courcelle 1991), which is a subfamily of HRL, as a possible alternative. We provide a top-down parsing algorithm for RGL that runs in time linear in the size of the input graph.

- title: "Semantic graph parsing with recurrent neural network DAG grammars"
  authors: Federico Fancellu, Sorcha Gilroy, Adam Lopez, Mirella Lapata
  year: 2019 
  month:  9
  type: journal 
  published: arXiv preprint arXiv:1910.00051
  abstract: >

- title: "Probabilistic graph formalisms for meaning representations"
  authors: Sorcha Gilroy
  year: 2019 
  month:  7
  publisher: The University of Edinburgh
  abstract: >

- title: "The problem with probabilistic DAG automata for semantic graphs"
  authors: Ieva Vasiljeva, Sorcha Gilroy, Adam Lopez
  year: 2018 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1810.12266
  abstract: >

- title: "(Re) introducing Regular Graph Languages"
  authors: Sorcha Gilroy, Adam Lopez, Sebastian Maneth, Pijus Simonaitis
  year: 2017/7
  type: conference 
  published: Proceedings of the 15th Meeting on the Mathematics of Language
  pages: 100-113
  abstract: >
Distributions over strings and trees can be represented by probabilistic regular languages, which characterise many models in natural language processing. Recently, several datasets have become available which represent natural language phenomena as graphs, so it is natural to ask whether there is an equivalent of probabilistic regular languages for graphs. This paper presents regular graph languages, a formalism due to Courcelle (1991) that has not previously been studied in natural language processing. RGL is crucially a subfamily of both Hyperedge Replacement Languages (HRL), which can be made probabilistic; and Monadic Second Order Languages (MSOL), which are closed under intersection. We give an accessible introduction to Courcelle&#8217;s proof that RGLs are in MSOL, providing clues about how RGL may relate to other recently introduced graph grammar formalisms.

- title: "Graph Formalisms for Meaning Representations"
  authors: Adam Lopez, Sorcha Gilroy
  year: 2018
  type: conference 
  published: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts
  abstract: >
In this tutorial we will focus on Hyperedge Replacement Languages (HRL; Drewes et al. 1997), a context-free graph rewriting system. HRL are one of the most popular graph formalisms to be studied in NLP (Chiang et al., 2013; Peng et al., 2015; Bauer and Rambow, 2016). We will discuss HRL by formally defining them, studying several examples, discussing their properties, and providing exercises for the tutorial. While HRL have been used in NLP in the past, there is some speculation that they are more expressive than is necessary for graphs representing natural language (Drewes, 2017). Part of our own research has been exploring what restrictions of HRL could yield languages that are more useful for NLP and also those that have desirable properties for NLP models, such as being closed under intersection. With that in mind, we also plan to discuss Regular Graph Languages (RGL; Courcelle 1991), a subfamily of HRL which are closed under intersection. The definition of RGL is relatively simple after being introduced to HRL. We do not plan on discussing any proofs of why RGL are also a subfamily of MSOL, as described in Gilroy et al.(2017b). We will briefly mention the other formalisms shown in Figure 1 such as MSOL and DAGAL but this will focus on their properties rather than any formal definitions.

- title: "Grand: Graph neural diffusion"
  authors: Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, Emanuele Rossi
  year: 2021 
  month:  7
  type: conference 
  published: International Conference on Machine Learning
  pages: 1407-1418
  publisher: PMLR
  abstract: >
We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.

- title: "Automatic Reparameterisation of Probabilistic Programs"
  authors: Maria I Gorinova, Dave Moore, Matthew D Hoffman
  year: 2020
  type: conference 
  published: Proceedings of International Conference on Machine Learning (ICML)
  abstract: >
Probabilistic programming has emerged as a powerful paradigm in statistics, applied science, and machine learning: by decoupling modelling from inference, it promises to allow modellers to directly reason about the processes generating data. However, the performance of inference algorithms can be dramatically affected by the parameterisation used to express a model, requiring users to transform their programs in non-intuitive ways. We argue for automating these transformations, and demonstrate that mechanisms available in recent modelling frameworks can implement non-centring and related reparameterisations. This enables new inference algorithms, and we propose two: a simple approach using interleaved sampling and a novel variational formulation that searches over a continuous space of parameterisations. We show that these approaches enable robust inference across a range of models, and can yield more efficient samplers than the best fixed parameterisation.

- title: "Probabilistic programming with densities in SlicStan: efficient, flexible, and deterministic"
  authors: Maria I Gorinova, Andrew D Gordon, Charles Sutton
  year: 2019 
  month:  1
  type: journal 
  published: Proceedings of the ACM on Programming Languages
  volume: 3<div class="gsc_oci_field">IssuePOPL
  pages: 1-30
  publisher: ACM
  abstract: >
Stan is a probabilistic programming language that has been increasingly used for real-world scalable projects. However, to make practical inference possible, the language sacrifices some of its usability by adopting a block syntax, which lacks compositionality and flexible user-defined functions. Moreover, the semantics of the language has been mainly given in terms of intuition about implementation, and has not been formalised.  

- title: "Effect handling for composable program transformations in Edward2"
  authors: Dave Moore, Maria I Gorinova
  year: 2018 
  month:  11
  type: journal 
  published: 2019 International Conference on Probabilistic Programming (PROBPROG)
  abstract: >

- title: "A live, multiple-representation probabilistic programming environment for novices"
  authors: Maria I Gorinova, Advait Sarkar, Alan F Blackwell, Don Syme
  year: 2016 
  month:  5
  type: conference 
  published: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
  pages: 2533-2537
  abstract: >
We present a live, multiple-representation novice environment for probabilistic programming based on the Infer .NET language. When compared to a text-only editor in a controlled experiment on 16 participants, our system showed a significant reduction in keystrokes during introductory probabilistic programming exercises, and subsequently, a significant improvement in program description and debugging tasks as measured by task time, keystrokes and deletions.

- title: "On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features"
  authors: Emanuele Rossi, Henry Kenlay, Maria I Gorinova, Benjamin Paul Chamberlain, Xiaowen Dong, Michael Bronstein
  year: 2021 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:2111.12128
  abstract: >

- title: "Conditional independence by typing"
  authors: 
  year: 2021 
  month:  12
  type: journal 
  published: ACM Transactions on Programming Languages and Systems (TOPLAS)
  volume: 44<div class="gsc_oci_field">Issue1
  pages: 1-54
  publisher: ACM
  abstract: >
A central goal of probabilistic programming languages (PPLs) is to separate modelling from inference. However, this goal is hard to achieve in practice. Users are often forced to re-write their models to improve efficiency of inference or meet restrictions imposed by the PPL. Conditional independence (CI) relationships among parameters are a crucial aspect of probabilistic models that capture a qualitative summary of the specified model and can facilitate more efficient inference. We present an information flow type system for probabilistic programming that captures conditional independence (CI) relationships and show that, for a well-typed program in our system, the distribution it implements is guaranteed to have certain CI-relationships. Further, by using type inference, we can statically <i>deduce</i> which CI-properties are present in a specified model. 

- title: "Predicting gaming related properties from twitter profiles"
  authors: Alfredo Kalaitzis, Maria Ivanova Gorinova, Yoad Lewenberg, Yoram Bachrach, Michael Fagan, Dean Carignan, Nitin Gautam
  year: 2016 
  month:  3
  type: conference 
  published: 2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)
  pages: 28-35
  publisher: IEEE
  abstract: >

- title: "Transforming spreadsheets with data noodles"
  authors: Maria I Gorinova, Advait Sarkar, Alan F Blackwell, Karl Prince
  year: 2016 
  month:  9
  type: conference 
  published: 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
  pages: 236-237
  publisher: IEEE
  abstract: >

- title: "Predicting gaming related properties from twitter accounts"
  authors: Maria Ivanova Gorinova, Yoad Lewenberg, Yoram Bachrach, Alfredo Kalaitzis, Michael Fagan, Dean Carignan, Nitin Gautam
  year: 2016 
  month:  3
  type: conference 
  published: Thirtieth AAAI Conference on Artificial Intelligence
  abstract: >
We demonstrate a system for predicting gaming related properties from Twitter accounts. Our system predicts various traits of users based on the tweets publicly available in their profiles. Such inferred traits include degrees of tech-savviness and knowledge on computer games, actual gaming performance, preferred platform, degree of originality, humor and influence on others. Our system is based on machine learning models trained on crowd-sourced data. It allows people to select Twitter accounts of their fellow gamers, examine the trait predictions made by our system, and the main drivers of these predictions. We present empirical results on the performance of our system based on its accuracy on our crowd-sourced dataset.

- title: "The end-user programming challenge of data wrangling"
  authors: MI Gorinova, Karl Prince, Sallyanne Meakins, Alain Vuylsteke, Matthew Jones, AF Blackwell
  year: 2016
  type: journal 
  published: Proceedings of the 27th annual workshop of the Psychology of Programming Interest Group (PPIG)
  pages: 140-149
  abstract: >
We present a case study of requirements for &#8220;data wrangling&#8221; capabilities in a healthcare application context. Data wrangling is an increasingly common requirement for data scientists, policy makers, market researchers, intelligence analysts, and other professions where existing data must be used in ways that were not envisioned when it was first collected. We characterise data wrangling as a programming problem, in which aggregate data must be restructured in ways that remain consistent with its semantic origins or ontological referents. We recommend the table as a lowest common denominator representational device, affording both direct manipulation and programming by example. We describe work in progress, in which we have identified new opportunities for clinical end-users to interact with the content of a customisable information system, through a focus on tables as an approachable analytic tool.

- title: "Usability of Probabilistic Programming Languages."
  authors: Alan F Blackwell, Luke Church, Martin Erwig, James Geddes, Andy Gordon, I Gorinova Maria, Atilim Gunes Baydin, Bradley Gram-Hansen, Tobias Kohn, Neil Lawrence, Vikash Mansinghka, Brooks Paige, Tomas Petricek, Diana Robinson, Advait Sarkar, Oliver Strickson
  year: 2019
  type: conference 
  published: PPIG
  abstract: >
This discussion paper presents a conversation between researchers having active interests in the usability of probabilistic programming languages (PPLs), but coming from a wide range of technical and research perspectives. Although PPL development is currently a vigorous and active research field, there has been very little attention to date to basic questions in the psychology of programming. Relevant issues include mental models associated with Bayesian probability, end-user applications of PPLs, the potential for data-first interaction styles, visualisation or explanation of model structure and solver behaviour, and many others.

- title: "Probabilistic Programming with SlicStan"
  authors: Maria I Gorinova
  year: 2017 
  month:  8
  institution: Master&#8217;s thesis. University of Edinburgh
  abstract: >
Probabilistic programming languages provide a concise and abstract way to specify probabilistic models, while hiding away the complicated underlying inference algorithm. However, those languages are often either not efficient enough to use in practice, or restrict the range of supported models and require understanding of how the compiled program is executed. Work in the programming languages community addresses this by attempting to use techniques such as program analysis and program abstraction, to improve the efficiency of the inference method used. However, such techniques have been restricted mainly within the community, and have not been applied to real-world probabilistic programming languages that have a large user-base. This work seeks ways in which programming language and static analysis techniques can be used to improve the increasingly mature probabilistic language Stan. We design and implement SlicStan&#8212;a probabilistic programming language that uses methods from the programming languages community in a novel way, to allow for more abstract and flexible Stan models. We show with examples the functionality of SlicStan, and report compiler performance results to show that our method is practical. This work demonstrates that black-box efficient inference can be the result of joint efforts between programming language and machine learning researchers. i

- title: "SlicStan: A Blockless Stan-like Language"
  authors: Maria I Gorinova, Andrew D Gordon, Charles Sutton
  year: 2018
  publisher: StanCon
  abstract: >
In Stan, data needs to be defined in one particular block, parameters in another; blocks must appear in order; and changing the block a variable is defined in could result in a semantically equivalent, but more or less efficient program. Declaring variables in the most suitable block could be challenging, especially for inexperienced users. Moreover, determining what that most suitable block is requires knowledge about the implementation of the underlying inference algorithm, as the block allocation is not always intuitive. For example, in the model from the previous subsection, the hyperparameters alpha and beta are declared as transformed data.

- title: "Program Analysis of Probabilistic Programs"
  authors: Maria I Gorinova
  year: 2022 
  month:  4
  type: journal 
  published: arXiv preprint arXiv:2204.06868
  abstract: >

- title: "Interactive Development Environment for Probabilistic Programming"
  authors: Maria I Gorinova
  year: 2015 
  month:  5
  abstract: >
&#8220;Graphical elegance is often found in simplicity of design and complexity of data.&#8221;&#8212;Edward TufteThis dissertation describes the process of planning, creation and evaluation of an interactive development environment for the probabilistic programming framework Infer .NET. It shows how the IDE was built to visualise underlying graphical models and random variables&#8217; distributions and, moreover, how that was done so the environment is live, ie with realtime interactivity.

- title: "GRAND: Graph Neural Diffusion Supplementary Material"
  authors: Benjamin P Chamberlain, James Rowbottom, Maria Gorinova, Stefan Webb, Emanuele Rossi, Michael M Bronstein
  abstract: >
(1) where W and a are learned and is the concatenation operator. However, for all datasets, the scaled dot product attention performed better. This may be because GAT relies on dropout. Dropout performs poorly inside adaptive timestep numerical ODE solvers as the stochasticity in the forward pass drives &#964;&#8594; 0.

- title: "Using generative modelling to produce varied intonation for speech synthesis"
  authors: Zack Hodari, Oliver Watts, Simon King
  year: 2019 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1906.04233
  abstract: >

- title: "Learning Interpretable Control Dimensions for Speech Synthesis by Using External Data."
  authors: Zack Hodari, Oliver Watts, Srikanth Ronanki, Simon King
  year: 2018 
  month:  9
  type: conference 
  published: Interspeech
  pages: 32-36
  abstract: >
There are many aspects of speech that we might want to control when creating text-to-speech (TTS) systems. We present a general method that enables control of arbitrary aspects of speech, which we demonstrate on the task of emotion control. Current TTS systems use supervised machine learning and are therefore heavily reliant on labelled data. If no labels are available for a desired control dimension, then creating interpretable control becomes challenging. We introduce a method that uses external, labelled data (ie not the original data used to train the acoustic model) to enable the control of dimensions that are not labelled in the original data. Adding interpretable control allows the voice to be manually controlled to produce more engaging speech, for applications such as audiobooks. We evaluate our method using a listening test.

- title: "CAMP: A two-stage approach to modelling prosody in context"
  authors: Zack Hodari, Alexis Moinet, Sri Karlapati, Jaime Lorenzo-Trueba, Thomas Merritt, Arnaud Joly, Ammar Abbas, Penny Karanasou, Thomas Drugman
  year: 2021 
  month:  6
  type: conference 
  published: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  pages: 6578-6582
  publisher: IEEE
  abstract: >

- title: "Prosodic representation learning and contextual sampling for neural text-to-speech"
  authors: Sri Karlapati, Ammar Abbas, Zack Hodari, Alexis Moinet, Arnaud Joly, Penny Karanasou, Thomas Drugman
  year: 2021 
  month:  6
  type: conference 
  published: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  pages: 6573-6577
  publisher: IEEE
  abstract: >

- title: "Investigating the Robustness of Sequence-to-Sequence Text-to-Speech Models to Imperfectly-Transcribed Training Data."
  authors: Jason Fong, Pilar Oplustil Gallegos, Zack Hodari, Simon King
  year: 2019
  type: conference 
  published: INTERSPEECH
  pages: 1546-1550
  abstract: >

- title: "Perception of prosodic variation for speech synthesis using an unsupervised discrete representation of F0"
  authors: Zack Hodari, Catherine Lai, Simon King
  year: 2020 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:2003.06686
  abstract: >

- title: "A learned emotion space for emotion recognition and emotive speech synthesis"
  authors: Zack Hodari
  year: 2017
  institution: Master&#8217;s thesis, The University of Edinburgh
  abstract: >
Emotion is a complex phenomenon that contributes heavily to human communication. Typically, human-computer interaction and text-to-speech systems do not account for emotion information, possibly due to lack of accurate emotion recognition and emotive speech synthesis methods. It seems likely that emotion recognition and synthesis has the ability to greatly improve how humans interface with machines.

- title: "Synthesising prosody with insufficient context"
  authors: Zack Hodari
  year: 2022 
  month:  10
  publisher: The University of Edinburgh
  abstract: >

- title: "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video"
  authors: Miguel Jaques, Michael Burke, Timothy Hospedales
  year: 2019 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:1905.11169
  abstract: >

- title: "Efficient variational bayesian neural network ensembles for outlier detection"
  authors: Nick Pawlowski, Miguel Jaques, Ben Glocker
  year: 2017 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:1703.06749
  abstract: >

- title: "Newtonianvae: Proportional control and goal identification from pixels via physical latent spaces"
  authors: Miguel Jaques, Michael Burke, Timothy M Hospedales
  year: 2021
  type: conference 
  published: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  pages: 4454-4463
  abstract: >
Learning low-dimensional latent state space dynamics models has proven powerful for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of simple and well-known PID controllers. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration. Notably, such proportional controlability also allows for robust path following from visual demonstrations using Dynamic Movement Primitives in the learned latent space.

- title: "Vision-based system identification and 3D keypoint discovery using dynamics constraints"
  authors: Miguel Jaques, Martin Asenov, Michael Burke, Timothy Hospedales
  year: 2021 
  month:  9
  type: journal 
  published: arXiv preprint arXiv:2109.05928
  abstract: >

- title: "Conditional Bayesian Neural Networks for Few-Shot Learning"
  authors: Miguel Jaques
  abstract: >
The recent advances in meta-learning-based few-shot learning have sparked interest in designing systems that can not only learn how to learn, but also produce uncertainty estimates conditioned on the task at hand. In this work we propose a K-shot N-way extenstion of the Learnet model, and generalise it to hierarchical Bayesian model, that we call Bayes-Learnet, which can produce a variational distribution over model parameters conditioned on the task. The model is trained by simple maximization of an evidence lower-bound using stochastic backpropagation. We show that Bayes-Learnet performs comparably to the state-of-the-art in K-shot N-way classification problems, and evaluate its ability to model uncertainty by applying it to regression, outlier detection and active learning problems. We also show, for the first time, that a single model trained for 1-shot learning generalises correctly at test time to any number of shots and ways, and propose a novel order-invariant data aggregation method. 

- title: "Teaching Machines to Paint"
  authors: Miguel Jaques
  abstract: >
The success of neural network-based methods has enabled the development of systems that emulate many human activities. In this work we approach the act of painting, and create a model that is able to reconstruct images by means of brushes or strokes, using a limited set of tools. To this end, the Painter network is developed. This network architecture is shown to be able to handle a variety of tasks, from simple images containing alphabet characters to larger and more complex images, like faces or natural scenes. The network is made end-toend differentiable by using differentiable attention mechanisms like spatial transformers [1] and creating a technique to allow discrete actions to be taken in a differentiable way. Additionally, we show that it is possible to create an occlusion-aware system that improves the model&#8217;s ability to understand scenes containing occlusions. We also dedicate a substantial portion of this work to showing that while sequential variational models like DRAW [2] and AIR [3] achieve good performance in their original tasks, they do not form an appropriate basis for a neural network-based painting model. 

- title: "scNMT-seq enables joint profiling of chromatin accessibility DNA methylation and transcription in single cells"
  authors: Stephen J Clark, Ricard Argelaguet, Chantriolnt-Andreas Kapourani, Thomas M Stubbs, Heather J Lee, Celia Alda-Catalinas, Felix Krueger, Guido Sanguinetti, Gavin Kelsey, John C Marioni, Oliver Stegle, Wolf Reik
  year: 2018 
  month:  2
  type: journal 
  published: Nature communications
  volume: 9<div class="gsc_oci_field">Issue1
  pages: 781
  publisher: Nature Publishing Group
  abstract: >
Parallel single-cell sequencing protocols represent powerful methods for investigating regulatory relationships, including epigenome-transcriptome interactions. Here, we report a single-cell method for parallel chromatin accessibility, DNA methylation and transcriptome profiling. scNMT-seq (single-cell nucleosome, methylation and transcription sequencing) uses a GpC methyltransferase to label open chromatin followed by bisulfite and RNA sequencing. We validate scNMT-seq by applying it to differentiating mouse embryonic stem cells, finding links between all three molecular layers and revealing dynamic coupling between epigenomic layers during differentiation.

- title: "Multi-omics profiling of mouse gastrulation at single-cell resolution"
  authors: 
  year: 2019/12
  type: journal 
  published: Nature
  volume: 576<div class="gsc_oci_field">Issue7787
  pages: 487-491
  publisher: Nature Publishing Group
  abstract: >

- title: "Glioblastomas acquire myeloid-affiliated transcriptional programs via epigenetic immunoediting to elicit immune evasion"
  authors: 
  year: 2021 
  month:  4
  type: journal 
  published: Cell
  volume: 184<div class="gsc_oci_field">Issue9
  pages: 2454-2470. e26
  publisher: Cell Press
  abstract: >

- title: "Melissa: Bayesian clustering and imputation of single-cell methylomes"
  authors: Chantriolnt-Andreas Kapourani, Guido Sanguinetti
  year: 2019/12
  type: journal 
  published: Genome biology
  volume: 20<div class="gsc_oci_field">Issue1
  pages: 1-15
  publisher: BioMed Central
  abstract: >

- title: "Higher order methylation features for clustering and prediction in epigenomic studies"
  authors: Chantriolnt-Andreas Kapourani, Guido Sanguinetti
  year: 2016 
  month:  9
  type: journal 
  published: Bioinformatics
  volume: 32<div class="gsc_oci_field">Issue17
  pages: i405-i412
  publisher: Oxford University Press
  abstract: >
DNA methylation is an intensely studied epigenetic mark, yet its functional role is incompletely understood. Attempts to quantitatively associate average DNA methylation to gene expression yield poor correlations outside of the well-understood methylation-switch at CpG islands.                               

- title: "scMET: Bayesian modeling of DNA methylation heterogeneity at single-cell resolution"
  authors: Chantriolnt-Andreas Kapourani, Ricard Argelaguet, Guido Sanguinetti, Catalina A Vallejos
  year: 2021/12
  type: journal 
  published: Genome biology
  volume: 22<div class="gsc_oci_field">Issue1
  pages: 1-21
  publisher: BioMed Central
  abstract: >

- title: "BPRMeth: a flexible Bioconductor package for modelling methylation profiles"
  authors: Chantriolnt-Andreas Kapourani, Guido Sanguinetti
  year: 2018 
  month:  3
  type: journal 
  published: Bioinformatics
  abstract: >
High-throughput measurements of DNA methylation are increasingly becoming a mainstay of biomedical investigations. While the methylation status of individual cytosines can sometimes be informative, several recent papers have shown that the functional role of DNA methylation is better captured by a quantitative analysis of the spatial variation of methylation across a genomic region.            

- title: "Extending the Social Network Interaction Model to Facilitate Collaboration through Service Provision"
  authors: Ourania Hatzi, Giannis Meletakis, Panagiotis Katsivelis, Andreas Kapouranis, Mara Nikolaidou, Dimosthenis Anagnostopoulos
  year: 2014
  type: conference 
  published: In Enterprise, Business-Process and Information Systems Modeling
  pages: pp. 94-108
  publisher: Springer Berlin Heidelberg
  abstract: >

- title: "Spatial statistical modelling of epigenomic variability"
  authors: Chantriolnt Andreas Kapourani
  year: 2019 
  month:  7
  publisher: The University of Edinburgh
  abstract: >

<div id="gsc_oci_title">Mixture Modelling of High-Throughput Biomedical Data"
  authors: Chantriolnt-Andreas Kapourani
  year: 2015
  institution: University of Edinburgh

- title: "Unsupervised Motif Discovery from Acoustic Time Series Data"
  authors: Chantriolnt-Andreas Kapourani
  year: 2013
  publisher: Technical report, Univerisity of Edinburgh, Edinburgh
  abstract: >

<div id="gsc_oci_title">Find Friend Location: Distributed real-time location tracking system for Android provided by Web Services"
  authors: Andreas-Chantriolnt Kapourani, Aitor Martin Marin, Juan Javier Garcia Cervera
  year: 2011 
  month:  1
  institution: Roskilde University

- title: "Extending the Social Network Interaction Model to Serve Collaboration in Enterprise 2.0"
  authors: Ourania Hatzi, Giannis Meletakis, Panagiotis Katsivelis, Andreas Kapouranis, Mara Nikolaidou, Dimosthenis Anagnostopoulos
  abstract: >

- title: "Big Code != Big Vocabulary: Open-vocabulary models for source code"
  authors: Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, Andrea Janes
  year: 2020 
  month:  6
  type: conference 
  published: Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings
  pages: 294-295
  abstract: >

- title: "How Often Do Single-Statement Bugs Occur? The ManySStuBs4J Dataset"
  authors: Rafael-Michael Karampatsis, Charles Sutton
  year: 2019 
  month:  5
  type: conference 
  published: 17th International Conference on Mining Software Repositories (MSR 2020)
  abstract: >

- title: "Maybe Deep Neural Networks are the Best Choice for Modeling Source Code. CoRR abs/1903.05734 (2019)"
  authors: Rafael-Michael Karampatsis, Charles Sutton
  year: 2019
  type: journal 
  published: arXiv preprint arXiv:1903.05734
  abstract: >

- title: "Scelmo: Source code embeddings from language models"
  authors: Rafael-Michael Karampatsis, Charles Sutton
  year: 2020 
  month:  4
  type: journal 
  published: arXiv preprint arXiv:2004.13214
  abstract: >

- title: "AUEB: Two Stage Sentiment Analysis of Social Network Messages."
  authors: Rafael-Michael Karampatsis, John Pavlopoulos, Prodromos Malakasiotis
  year: 2014 
  month:  8
  type: conference 
  published: SemEval@ COLING
  pages: 114-118
  abstract: >
This paper describes the system submitted for the Sentiment Analysis in Twitter Task of SEMEVAL 2014 and specifically the Message Polarity Classification subtask. We used a 2&#8211;stage pipeline approach employing a linear SVM classifier at each stage and several features including morphological features, POS tags based features and lexicon based features.

- title: "CDTDS: Predicting paraphrases in Twitter via support vector regression"
  authors: Rafael-Michael Karampatsis
  year: 2015/6
  type: conference 
  published: Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)
  pages: 75-79
  abstract: >
In this paper we describe a system that recognizes paraphrases in Twitter for tweets that refer to the same topic. The system participated in Task1 of SEMEVAL-2015 and uses a support vector regression machine to predict the degree of similarity. The similarity is then thresholded to create a binary prediction. The model and experimental results are discussed along with future work that could improve the method.

- title: "Multiplexnet: Towards fully satisfied logical constraints in neural networks"
  authors: Nick Hoernle, Rafael Michael Karampatsis, Vaishak Belle, Kobi Gal
  year: 2022 
  month:  6
  type: journal 
  published: Proceedings of the AAAI Conference on Artificial Intelligence
  volume: 36<div class="gsc_oci_field">Issue5
  pages: 5700-5709
  abstract: >
We propose a novel way to incorporate expert knowledge into the training of deep neural networks. Many approaches encode domain constraints directly into the network architecture, requiring non-trivial or domain-specific engineering. In contrast, our approach, called MultiplexNet, represents domain knowledge as a quantifier-free logical formula in disjunctive normal form (DNF) which is easy to encode and to elicit from human experts. It introduces a latent Categorical variable that learns to choose which constraint term optimizes the error function of the network and it compiles the constraints directly into the output of existing learning algorithms. We demonstrate the efficacy of this approach empirically on several classical deep learning tasks, such as density estimation and classification in both supervised and unsupervised settings where prior knowledge about the domains was expressed as logical constraints. Our results show that the MultiplexNet approach learned to approximate unknown distributions well, often requiring fewer data samples than the alternative approaches. In some cases, MultiplexNet finds better solutions than the baselines; or solutions that could not be achieved with the alternative approaches. Our contribution is in encoding domain knowledge in a way that facilitates inference. We specifically focus on quantifier-free logical formulae that are specified over the output domain of a network. We show that this approach is both efficient and general; and critically, our approach guarantees 100% constraint satisfaction in a network&#39;s output.

- title: "nlp. cs. aueb. gr: Two stage sentiment analysis"
  authors: Prodromos Malakasiotis, Rafael-Michael Karampatsis, Konstantina Makrynioti, John Pavlopoulos
  year: 2013/6
  type: conference 
  published: Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)
  pages: 562-567
  abstract: >
This paper describes the systems with which we participated in the task Sentiment Analysis in Twitter of SEMEVAL 2013 and specifically the Message Polarity Classification. We used a 2-stage pipeline approach employing a linear SVM classifier at each stage and several features including BOW features, POS based features and lexicon based features. We have also experimented with Naive Bayes classifiers trained with BOW features.

<div id="gsc_oci_title">Translating Natural Language into Source Code Via Tree Transduction MRes Thesis"
  authors: Rafael-Michael Karampatsis
  year: 2015
  institution: University of Edinburgh

- title: "Signal Perceptron: On the Identifiability of Boolean Function Spaces and Beyond"
  authors: Miguel-Angel Mendez Lucero, Rafael-Michael Karampatsis, Enrique Bojorquez Gallardo, Vaishak Belle
  year: 2022
  type: journal 
  published: Frontiers in Artificial Intelligence
  pages: 108
  publisher: Frontiers
  abstract: >

- title: "Scalable deep learning for bug detection"
  authors: Rafael-Michael Karampatsis
  year: 2021 
  month:  7
  institution: The University of Edinburgh
  abstract: >

<div id="gsc_oci_title">Social Media Sentiment Analysis MSc Thesis"
  authors: Rafael-Michael Karampatsis
  year: 2014
  institution: Athens University of Economics and Bussiness

- title: "How Often Do Single-Statement Bugs Occur?"
  authors: Rafael-Michael Karampatsis, Charles Sutton

- title: "Message from the MSR 2021 Mining Challenge Track Co-Chairs"
  authors: Miltiadis Allamanis, Rafael-Michael Karampatsis, Charles Sutton
  abstract: >
The Mining Software Repositories (MSR) challenge is a long-standing tradition, dating back to 2006. It is open to all researchers in the field, and frequently participated in by young researchers and motivated students. MSR 2021 has been no exception, with the MSR conference holding the 16th edition of the challenge, which we have been honored to chair.

- title: "Bayesian experimental design for implicit models by mutual information neural estimation"
  authors: Steven Kleinegesse, Michael U Gutmann
  year: 2020 
  month:  11
  type: conference 
  published: International Conference on Machine Learning
  pages: 5316-5326
  publisher: PMLR
  abstract: >
Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.

- title: "Efficient Bayesian experimental design for implicit models"
  authors: Steven Kleinegesse, Michael U Gutmann
  year: 2019 
  month:  4
  type: conference 
  published: The 22nd International Conference on Artificial Intelligence and Statistics
  pages: 476-485
  publisher: PMLR
  abstract: >
Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search or sampling-based methods. We find that this increases efficiency and allows us to consider higher design dimensions.

- title: "Recognizing emotions in video using multimodal dnn feature fusion"
  authors: Jennifer Williams, Steven Kleinegesse, Ramona Comanescu, Oana Radu
  year: 2018/7
  type: conference 
  published: Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)
  pages: 11-19
  abstract: >
We present our system description of input-level multimodal fusion of audio, video, and text for recognition of emotions and their intensities for the 2018 First Grand Challenge on Computational Modeling of Human Multimodal Language. Our proposed approach is based on input-level feature fusion with sequence learning from Bidirectional Long-Short Term Memory (BLSTM) deep neural networks (DNNs). We show that our fusion approach outperforms unimodal predictors. Our system performs 6-way simultaneous classification and regression, allowing for overlapping emotion labels in a video segment. This leads to an overall binary accuracy of 90%, overall 4-class accuracy of 89.2% and an overall mean-absolute-error (MAE) of 0.12. Our work shows that an early fusion technique can effectively predict the presence of multi-label emotions as well as their coarse-grained intensities. The presented multimodal approach creates a simple and robust baseline on this new Grand Challenge dataset. Furthermore, we provide a detailed analysis of emotion intensity distributions as output from our DNN, as well as a related discussion concerning the inherent difficulty of this task.

- title: "Pulse shape discrimination and exploration of scintillation signals using convolutional neural networks"
  authors: J Griffiths, Steven Kleinegesse, D Saunders, R Taylor, Antonin Vacheret
  year: 2020 
  month:  10
  type: journal 
  published: Machine Learning: Science and Technology
  volume: 1<div class="gsc_oci_field">Issue4
  pages: 045022
  publisher: IOP Publishing
  abstract: >

- title: "Implicit deep adaptive design: policy-based experimental design without likelihoods"
  authors: Desi R Ivanova, Adam Foster, Steven Kleinegesse, Michael U Gutmann, Thomas Rainforth
  year: 2021 
  month:  12
  type: journal 
  published: Advances in Neural Information Processing Systems
  volume: 34
  pages: 25785-25798
  abstract: >
We introduce implicit Deep Adaptive Design (iDAD), a new method for performing adaptive experiments in real-time with implicit models. iDAD amortizes the cost of Bayesian optimal experimental design (BOED) by learning a design policy network upfront, which can then be deployed quickly at the time of the experiment. The iDAD network can be trained on any model which simulates differentiable samples, unlike previous design policy work that requires a closed form likelihood and conditionally independent experiments. At deployment, iDAD allows design decisions to be made in milliseconds, in contrast to traditional BOED approaches that require heavy computation during the experiment itself. We illustrate the applicability of iDAD on a number of experiments, and show that it provides a fast and effective mechanism for performing adaptive design with implicit models.

- title: "Gradient-based Bayesian experimental design for implicit models using mutual information lower bounds"
  authors: Steven Kleinegesse, Michael U Gutmann
  year: 2021 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:2105.04379
  abstract: >

- title: "Statistical applications of contrastive learning"
  authors: Michael U Gutmann, Steven Kleinegesse, Benjamin Rhodes
  year: 2022 
  month:  6
  type: journal 
  published: Behaviormetrika
  pages: 1-25
  publisher: Springer Japan
  abstract: >
The likelihood function plays a crucial role in statistical inference and experimental design. However, it is computationally intractable for several important classes of statistical models, including energy-based models and simulator-based models. Contrastive learning is an intuitive and computationally feasible alternative to likelihood-based learning. We here first provide an introduction to contrastive learning and then show how we can use it to derive methods for diverse statistical problems, namely parameter estimation for energy-based models, Bayesian inference for simulator-based models, as well as experimental design.

- title: "Domain Knowledge in A*-Based Causal Discovery"
  authors: Steven Kleinegesse, Andrew R Lawrence, Hana Chockler
  year: 2022 
  month:  8
  type: journal 
  published: arXiv preprint arXiv:2208.08247
  abstract: >

- title: "Bayesian Optimal Experimental Design for Simulator Models of Cognition"
  authors: Simon Valentin, Steven Kleinegesse, Neil R Bramley, Michael U Gutmann, Christopher G Lucas
  year: 2021 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2110.15632
  abstract: >

- title: "Efficient Automated Online Experimentation with Multi-Fidelity"
  authors: Steven Kleinegesse, Zhenwen Dai, Andreas Damianou, Kamil Ciosek, Federico Tomasi
  year: 2021 
  month:  9
  type: conference 
  published: Fifth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems
  abstract: >
Prominent online experimentation approaches in industry, such as A/B testing, are often not scalable with respect to the number of candidate models. To address this shortcoming, recent work has introduced an automated online experimentation (AOE) scheme that uses a probabilistic model of user behavior to predict online performance of candidate models. While effective, these predictions of online performance may be biased due to various unforeseen circumstances, such as user modelling bias, a shift in data distribution or an incomplete set of features. In this work, we leverage advances from multi-fidelity optimization in order to combine AOE with Bayesian optimization (BO). This mitigates the effect of biased predictions, while still retaining scalability and performance. Furthermore, our approach also allows us to optimally adjust the number of users in a test cell, which is typically kept constant for online experimentation schemes, leading to a more effective allocation of resources. Our synthetic experiments show that our method yields improved performance, when compared to AOE, BO and other baseline approaches.

- title: "Bayesian Experimental Design for Intractable Models of Cognition"
  authors: Simon Valentin, Steven Kleinegesse, Neil R Bramley, Michael Gutmann, Chris Lucas
  year: 2021
  type: journal 
  published: Proceedings of the Annual Meeting of the Cognitive Science Society
  volume: 43<div class="gsc_oci_field">Issue43
  abstract: >

- title: "Recognizing emotions in video using multimodal dnn feature fusion Open Website"
  authors: Ramona Comanescu, Jennifer Williams, Steven Kleinegesse
  abstract: >

- title: "Relational inductive biases, deep learning, and graph networks"
  authors: Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu
  year: 2018 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1806.01261
  abstract: >

- title: "Efficient graph generation with graph recurrent attention networks"
  authors: Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel Urtasun, Richard Zemel
  year: 2019
  type: journal 
  published: Advances in neural information processing systems
  volume: 32
  abstract: >
We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. Our code is released at:\url {https://github. com/lrjconan/GRAN}.

- title: "The shape variational autoencoder: A deep generative model of part&#8208;segmented 3D objects"
  authors: Charlie Nash, Christopher KI Williams
  year: 2017/8
  type: journal 
  published: Computer Graphics Forum
  volume: 36<div class="gsc_oci_field">Issue5
  pages: 1-12
  abstract: >

- title: "Polygen: An autoregressive generative model of 3d meshes"
  authors: Charlie Nash, Yaroslav Ganin, SM Ali Eslami, Peter Battaglia
  year: 2020 
  month:  11
  type: conference 
  published: International conference on machine learning
  pages: 7220-7229
  publisher: PMLR
  abstract: >
Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches for object synthesis have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present PolyGen, a generative model of 3D objects which models the mesh directly, predicting vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.

- title: "Relational inductive biases, deep learning, and graph networks. arXiv 2018"
  authors: Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andy Ballard, Justin Gilmer, George E Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu
  year: 2018
  type: journal 
  published: arXiv preprint arXiv:1806.01261

- title: "Autoregressive energy machines"
  authors: Charlie Nash, Conor Durkan
  year: 2019 
  month:  5
  type: conference 
  published: International Conference on Machine Learning
  pages: 1735-1744
  publisher: PMLR
  abstract: >
Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.

- title: "Relational inductive biases, deep learning, and graph networks. arXiv"
  authors: PW Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, V Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, C Gulcehre, F Song, A Ballard, J Gilmer, G Dahl, A Vaswani, K Allen, C Nash, V Langston, C Dyer, N Heess, D Wierstra, P Kohli, M Botvinick, O Vinyals, Y Li, R Pascanu
  year: 2018
  type: journal 
  published: arXiv preprint arXiv:1806.01261

- title: "Overcoming occlusion with inverse graphics"
  authors: Pol Moreno, Christopher KI Williams, Charlie Nash, Pushmeet Kohli
  year: 2016 
  month:  10
  type: conference 
  published: European Conference on Computer Vision
  pages: 170-185
  publisher: Springer, Cham
  abstract: >

- title: "Generating images with sparse representations"
  authors: Charlie Nash, Jacob Menick, Sander Dieleman, Peter W Battaglia
  year: 2021 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:2103.03841
  abstract: >

- title: "The multi-entity variational autoencoder"
  authors: Charlie Nash, SM Ali Eslami, Chris Burgess, Irina Higgins, Daniel Zoran, Theophane Weber, Peter Battaglia
  year: 2017
  type: journal 
  published: NIPS Workshops
  abstract: >
Representing the world as objects is core to human intelligence. It is the basis of people&#8217;s sophisticated capacities for reasoning, imagining, planning, and learning. Artificial intelligence typically assumes human-defined representations of objects, and little work has explored how object-based representations can arise through unsupervised learning. Here we present an approach for learning probabilistic, object-based representations from data, called the &#8220;multi-entity variational autoencoder&#8221;(MVAE), whose prior and posterior distributions are defined over a set of random vectors. We demonstrate that the model can learn interpretable representations of visual scenes that disentangle objects and their properties.

- title: "Relational inductive biases, deep learning, and graph networks. arXiv e-prints"
  authors: PW Battaglia, JB Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, C Gulcehre, F Song, A Ballard, J Gilmer, G Dahl, A Vaswani, K Allen, C Nash, V Langston, C Dyer, N Heess, D Wierstra, P Kohli, M Botvinick, O Vinyals, Y Li, R Pascanu
  year: 2018
  type: journal 
  published: arXiv preprint arXiv:1806.01261

- title: "Inverting supervised representations with autoregressive neural density models"
  authors: Charlie Nash, Nate Kushman, Christopher KI Williams
  year: 2019 
  month:  4
  type: conference 
  published: The 22nd International Conference on Artificial Intelligence and Statistics
  pages: 1620-1629
  publisher: PMLR
  abstract: >
We present a method for feature interpretation that makes use of recent advances in autoregressive density estimation models to invert model representations. We train generative inversion models to express a distribution over input features conditioned on intermediate model representations. Insights into the invariances learned by supervised models can be gained by viewing samples from these inversion models. In addition, we can use these inversion models to estimate the mutual information between a model&#8217;s inputs and its intermediate representations, thus quantifying the amount of information preserved by the network at different stages. Using this method we examine the types of information preserved at different layers of convolutional neural networks, and explore the invariances induced by different architectural choices. Finally we show that the mutual information between inputs and network layers initially increases and then decreases over the course of training, supporting recent work by Shwartz-Ziv and Tishby (2017) on the information bottleneck theory of deep learning.

- title: "Autoencoders and probabilistic inference with missing data: An exact solution for the factor analysis case"
  authors: 
  year: 2018 
  month:  1
  type: journal 
  published: arXiv preprint arXiv:1801.03851
  abstract: >

- title: "Variable-rate discrete representation learning"
  authors: Sander Dieleman, Charlie Nash, Jesse Engel, Karen Simonyan
  year: 2021 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:2103.06089
  abstract: >

- title: "HDMapGen: A hierarchical graph generative model of high definition maps"
  authors: Lu Mi, Hang Zhao, Charlie Nash, Xiaohan Jin, Jiyang Gao, Chen Sun, Cordelia Schmid, Nir Shavit, Yuning Chai, Dragomir Anguelov
  year: 2021
  type: conference 
  published: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  pages: 4227-4236
  abstract: >
High Definition (HD) maps are maps with precise definitions of road lanes with rich semantics of the traffic rules. They are critical for several key stages in an autonomous driving system, including motion forecasting and planning. However, there are only a small amount of real-world road topologies and geometries, which significantly limits our ability to test out the self-driving stack to generalize onto new unseen scenarios. To address this issue, we introduce a new challenging task to generate HD maps. In this work, we explore several autoregressive models using different data representations, including sequence, plain graph, and hierarchical graph. We propose HDMapGen, a hierarchical graph generation model capable of producing high-quality and diverse HD maps through a coarse-to-fine approach. Experiments on the Argoverse dataset and an in-house dataset show that HDMapGen significantly outperforms baseline methods. Additionally, we demonstrate that HDMapGen achieves high efficiency and scalability.

- title: "General-purpose, long-context autoregressive modeling with Perceiver AR"
  authors: 
  year: 2022 
  month:  2
  type: journal 
  published: arXiv preprint arXiv:2202.07765
  abstract: >

- title: "Transframer: Arbitrary Frame Prediction with Generative Models"
  authors: 
  year: 2022 
  month:  3
  type: journal 
  published: arXiv preprint arXiv:2203.09494
  abstract: >

- title: "Generative Entity Networks: Disentangling Entitites and Attributes in Visual Scenes using Partial Natural Language Descriptions"
  authors: Charlie Nash, Sebastian Nowozin, Nate Kushman
  year: 2018 
  month:  2
  abstract: >
Generative image models have made significant progress in the last few years, and are now able to generate low-resolution images which sometimes look realistic. However the state-of-the-art models utilize fully entangled latent representations where small changes to a single neuron can effect every output pixel in relatively arbitrary ways, and different neurons have possibly arbitrary relationships with each other. This limits the ability of such models to generalize to new combinations or orientations of objects as well as their ability to connect with more structured representations such as natural language, without explicit strong supervision. In this work explore the synergistic effect of using partial natural language scene descriptions to help disentangle the latent entities visible an image. We present a novel neural network architecture called Generative Entity Networks, which jointly generates both the natural language descriptions and the images from a set of latent entities. Our model is based on the variational autoencoder framework and makes use of visual attention to identify and characterise the visual attributes of each entity. Using the Shapeworld dataset, we show that our representation both enables a better generative model of images, leading to higher quality image samples, as well as creating more semantically useful representations that improve performance over purely dicriminative models on a simple natural language yes/no question answering task.

- title: "Generative models of part-structured 3D objects"
  authors: Charlie Nash, CK Williams
  abstract: >
We introduce two generative models of part-segmented 3D objects: the shape variational auto-encoder (ShapeVAE) and the shape factor analyzer (ShapeFA). These models describe a distribution over the co-existence of object parts, as well as over the continuous variability of the object surface, leveraging the part structure of 3D objects in their architecture. We demonstrate that while the ShapeFA slightly outperforms the ShapeVAE in terms of density estimation, the ShapeVAE produces better quality samples and is effective at completing partially obscured shapes.

- title: "Unsupervised learning with neural latent variable models"
  authors: Charlie Nash
  year: 2020
  institution: University of Edinburgh
  abstract: >

- title: "Masked autoregressive flow for density estimation"
  authors: George Papamakarios, Theo Pavlakou, Iain Murray
  year: 2017
  type: journal 
  published: Advances in neural information processing systems
  volume: 30
  abstract: >
Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.

- title: "Normalizing Flows for Probabilistic Modeling and Inference"
  authors: George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, Balaji Lakshminarayanan
  year: 2021<div class="gsc_oci_field">SourceJournal of Machine Learning Research
  volume: 22<div class="gsc_oci_field">Issue57
  pages: 1-64
  abstract: >
Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.

- title: "Neural spline flows"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2019
  type: conference 
  published: Advances in Neural Information Processing Systems
  pages: 7511-7522
  abstract: >
A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.

- title: "Fast &#949;-free inference of simulation models with Bayesian conditional density estimation"
  authors: George Papamakarios, Iain Murray
  year: 2016
  type: conference 
  published: Advances in Neural Information Processing Systems
  pages: 1028-1036
  abstract: >
Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an &#949;-ball around the observed data, which is only correct in the limit &#949;&#8594; 0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as &#949;&#8594; 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.

- title: "Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows"
  authors: George Papamakarios, David Sterratt, Iain Murray
  year: 2019 
  month:  4
  type: conference 
  published: The 22nd International Conference on Artificial Intelligence and Statistics
  pages: 837-848
  abstract: >
We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.

- title: "Temporal difference variational auto-encoder"
  authors: Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, Theophane Weber
  year: 2018 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1806.03107
  abstract: >

- title: "Normalizing flows on tori and spheres"
  authors: 
  year: 2020 
  month:  11
  type: conference 
  published: International Conference on Machine Learning
  pages: 8083-8092
  publisher: PMLR
  abstract: >
Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.

- title: "On contrastive learning for likelihood-free inference"
  authors: Conor Durkan, Iain Murray, George Papamakarios
  year: 2020 
  month:  11
  type: conference 
  published: International Conference on Machine Learning
  pages: 2771-2781
  publisher: PMLR
  abstract: >
Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classifier to distinguish between pairs of parameter-observation samples generated using the simulator and pairs sampled from some reference distribution, which implicitly learns a density ratio proportional to the likelihood. Another popular class of methods fits a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of flexible neural density estimators for this task. In this work, we show that both of these approaches can be unified under a general contrastive learning scheme, and clarify how they should be run and compared.

- title: "Cubic-Spline Flows"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2019 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:1906.02145
  abstract: >

- title: "Targeted free energy estimation via learned mappings"
  authors: 
  year: 2020 
  month:  10
  type: journal 
  published: The Journal of Chemical Physics
  volume: 153<div class="gsc_oci_field">Issue14
  pages: 144112
  publisher: AIP Publishing LLC
  abstract: >

- title: "The lipschitz constant of self-attention"
  authors: Hyunjik Kim, George Papamakarios, Andriy Mnih
  year: 2021 
  month:  7
  type: conference 
  published: International Conference on Machine Learning
  pages: 5562-5571
  publisher: PMLR
  abstract: >
Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.

- title: "The DeepMind JAX Ecosystem"
  authors: Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin, Chris Jones, Tom Hennigan, Matteo Hessel, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Lena Martens, Vladimir Mikulik, Tamara Norman, John Quan, George Papamakarios, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Wojciech Stokowiec, Fabio Viola
  year: 2020
  type: journal 
  published: URL http://github. com/deepmind

- title: "nflows: normalizing flows in PyTorch"
  authors: Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios
  year: 2020/11
  type: journal 
  published: Version v0
  volume: 14

- title: "Neural density estimation and likelihood-free inference"
  authors: George Papamakarios
  year: 2019 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:1910.13233
  abstract: >

- title: "Sequential Neural Methods for Likelihood-free Inference"
  authors: Conor Durkan, George Papamakarios, Iain Murray
  year: 2018 
  month:  11
  type: journal 
  published: arXiv preprint arXiv:1811.08723
  abstract: >

- title: "Neural belief states for partially observed domains"
  authors: Pol Moreno, Jan Humplik, George Papamakarios, Bernardo Avila Pires, Lars Buesing, Nicolas Heess, Theophane Weber
  year: 2018
  type: journal 
  published: NeurIPS 2018 workshop on Reinforcement Learning under Partial Observability
  abstract: >
An important challenge in reinforcement learning arises in domains where the agent&#8217;s observations are partial or noisy measurements of the state of the environment. In such domains, a policy that depends only on the current observation xt is generally suboptimal; an optimal policy must in principle depend on the entire history of observations and actions.

- title: "Causally Correct Partial Models for Reinforcement Learning"
  authors: Danilo J Rezende, Ivo Danihelka, George Papamakarios, Nan Rosemary Ke, Ray Jiang, Theophane Weber, Karol Gregor, Hamza Merzic, Fabio Viola, Jane Wang, Jovana Mitrovic, Frederic Besse, Ioannis Antonoglou, Lars Buesing
  year: 2020 
  month:  2
  type: journal 
  published: arXiv preprint arXiv:2002.02836
  abstract: >

- title: "Distilling model knowledge"
  authors: George Papamakarios
  year: 2015 
  month:  10<div class="gsc_oci_field">SourcearXiv preprint arXiv:1510.02437
  institution: University of Edinburgh
  abstract: >

- title: "Generalised scalable robust principal component analysis"
  authors: Georgios Papamakarios, Yannis Panagakis, Stefanos Zafeiriou
  year: 2014 
  month:  9
  pages: 116.1-116.11
  publisher: BMVA Press
  abstract: >
The robust estimation of the low-dimensional subspace that spans the data from a set of high-dimensional, possibly corrupted by gross errors and outliers observations is fundamental in many computer vision problems. The state-of-the-art robust principal component analysis (PCA) methods adopt convex relaxations of l0 quasi-norm-regularised rank minimisation problems. That is, the nuclear norm and the l1-norm are employed. However, this convex relaxation may make the solutions deviate from the original ones. To this end, the Generalised Scalable Robust PCA (GSRPCA) is proposed, by reformulating the robust PCA problem using the Schatten p-norm and the lq-norm subject to orthonormality constraints, resulting in a better non-convex approximation of the original sparsity regularised rank minimisation problem. It is worth noting that the common robust PCA variants are special cases of the GSRPCA when p= q= 1 and by properly choosing the upper bound of the number of the principal components. An efficient algorithm for the GSRPCA is developed. The performance of the GSRPCA is assessed by conducting experiments on both synthetic and real data. The experimental results indicate that the GSRPCA outperforms the common state-of-the-art robust PCA methods without introducing much extra computational cost.

- title: "Distilling intractable generative models"
  authors: George Papamakarios, Iain Murray
  year: 2015
  type: journal 
  published: Probabilistic Integration Workshop at Neural Information Processing Systems
  abstract: >
A generative model&#8217;s partition function is typically expressed as an intractable multi-dimensional integral, whose approximation presents a challenge to numerical and Monte Carlo integration. In this work, we propose a new estimation method for intractable partition functions, based on distilling an intractable generative model into a tractable approximation thereof, and using the latter for proposing Monte Carlo samples. We empirically demonstrate that our method produces state-of-the-art estimates, even in combination with simple Monte Carlo methods.

- title: "Masked autoregressive flow for density estimation"
  authors: George Papamakarios, Theo Pavlakou, Iain Murray
  year: 2017
  type: journal 
  published: Advances in neural information processing systems
  volume: 30
  abstract: >
Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.

- title: "Masked autoregressive flow for density estimation"
  authors: George Papamakarios, Theo Pavlakou, Iain Murray
  year: 2017
  type: journal 
  published: Advances in neural information processing systems
  volume: 30
  abstract: >
Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.

- title: "Advances in neural information processing systems"
  authors: G Papamakarios, I Murray, T Pavlakou
  year: 2016
  volume: 29
  pages: 1028-1036
  publisher: Curran Associates

- title: "Masked autoregressive flow for density estimation,(2018)"
  authors: George Papamakarios, Theo Pavlakou, Iain Murray
  type: journal 
  published: arXiv preprint arXiv:1705.07057

- title: "Masked Autoregressive Flow for Density Estimation, arXiv e-prints"
  authors: George Papamakarios, Theo Pavlakou, Iain Murray
  year: 2017/5
  type: journal 
  published: arXiv preprint arXiv:1705.07057
  volume: 1705

- title: "Masked Autoregressive Flow for Density Estimation. arXiv e-prints, page"
  authors: George Papamakarios, Theo Pavlakou, Iain Murray
  year: 2017/5
  type: journal 
  published: arXiv preprint arXiv:1705.07057

- title: "Masked Autoregressive Flow for Density Estimation"
  authors: George Papamakarios Iain Murray, Theo Pavlakou
  year: 2017
  type: journal 
  published: Advances in Neural Information Processing Systems
  abstract: >

- title: "Improving event recognition using Sparse PCA in the context of London Twitter data"
  authors: Theo Pavlakou, Arta Babaee, Moez Draief
  year: 2014
  type: conference 
  published: Information Sciences and Systems 2014
  pages: 357-365
  publisher: Springer, Cham
  abstract: >
Motivated by some of the recent work based on using sparse principal component analysis to analyse social media, we propose an improvement which involves altering the input data matrices by considering what relationships they represent. Accordingly, we confirm our result by using Twitter data from London in the year 2012 as a medium to demonstrate on. Various alterations are made to the data matrix obtained from this data and the resulting matrices are then passed through a sparse principal component analysis algorithm. The resulting outputs are then analysed and it is shown that indeed the results do differ, with one particular variation consistently outperforming the rest. Our results are especially of interest when the data to be analysed can be represented by a binary matrix of some sort, e.g. in document analysis.

- title: "Large Scale Optimisation Algorithms for Machine Learning"
  authors: Theo Pavlakou
  abstract: >

- title: "Towards automated classification of seabed substrates in underwater video"
  authors: Matthew Pugh, Bernard Tiddeman, Hannah Dee, Philip Hughes
  year: 2014 
  month:  8
  type: conference 
  published: 2014 ICPR Workshop on Computer Vision for Analysis of Underwater Imagery
  pages: 9-16
  publisher: IEEE
  abstract: >

- title: "Indico: A Collaboration Hub"
  authors: P Ferreira, T Baron, C Bossy, M Pugh, A Resco, J Trzaskoma, C Wachter
  year: 2012 
  month:  12
  type: journal 
  published: Journal of Physics: Conference Series
  volume: 396<div class="gsc_oci_field">Issue6
  pages: 062006
  publisher: IOP Publishing
  abstract: >

- title: "Indico 1.0"
  authors: 
  year: 2014 
  month:  6
  type: journal 
  published: Journal of Physics: Conference Series
  volume: 513<div class="gsc_oci_field">Issue6
  pages: 062020
  publisher: IOP Publishing
  abstract: >

- title: "The importance of awareness for understanding language."
  authors: Hugh Rabagliati, Alexander Robertson, David Carmel
  year: 2018/2
  type: journal 
  published: Journal of Experimental Psychology: General
  volume: 147<div class="gsc_oci_field">Issue2
  pages: 190
  publisher: American Psychological Association
  abstract: >

- title: "Self-Representation on Twitter Using Emoji Skin Color Modifiers"
  authors: Alexander Robertson, Walid Magdy, Sharon Goldwater
  year: 2018
  type: conference 
  published: ICWSM
  abstract: >
Since 2015, it has been possible to modify certain emoji with a skin tone. The five different skin tones were introduced with the aim of representing more human diversity, but some commentators feared they might be used as a way to negatively represent other users/groups. This paper presents a quantitative analysis of the use of skin tone modifiers on emoji on Twitter, showing that users with darker-skinned profile photos employ them more often than users with lighter-skinned profile photos, and the vast majority of skin tone usage matches the color of a user&#8217;s profile photo&#8212;ie, tones represent the self, rather than the other. In the few cases where users do use opposite-toned emoji, we find no evidence of negative racial sentiment. Thus, the introduction of skin tones seems to have met the goal of better representing human diversity.

- title: "How do children learn to avoid referential ambiguity? Insights from eye-tracking"
  authors: Hugh Rabagliati, Alexander Robertson
  year: 2017 
  month:  6
  type: journal 
  published: Journal of Memory and Language
  volume: 94
  pages: 15-27
  publisher: Academic Press
  abstract: >

- title: "Emoji skin tone modifiers: Analyzing variation in usage on social media"
  authors: Alexander Robertson, Walid Magdy, Sharon Goldwater
  year: 2020 
  month:  4
  type: journal 
  published: ACM Transactions on Social Computing
  volume: 3<div class="gsc_oci_field">Issue2
  pages: 1-25
  publisher: ACM
  abstract: >

- title: "The language of dialogue is complex"
  authors: Alexander Robertson, Luca Maria Aiello, Daniele Quercia
  year: 2019 
  month:  7
  type: journal 
  published: Proceedings of the International AAAI Conference on Web and Social Media
  volume: 13
  pages: 428-439
  abstract: >
Integrative Complexity (IC) is a psychometric that measures the ability of a person to recognize multiple perspectives and connect them, thus identifying paths for conflict resolution. IC has been linked to a wide variety of political, social and personal outcomes but evaluating it is a time-consuming process requiring skilled professionals to manually score texts, a fact which accounts for the limited exploration of IC at scale on social media. We combine natural language processing and machine learning to train an IC classification model that achieves state-of-the-art performance on unseen data and more closely adheres to the established structure of the IC coding process than previous automated approaches. When applied to the content of 400k+ comments from online fora about depression and knowledge exchange, our model was capable of replicating key findings of prior work, thus providing the first example of using IC tools for large-scale social media analytics.

- title: "Evaluating historical text normalization systems: How well do they generalize?"
  authors: Alexander Robertson, Sharon Goldwater
  year: 2018
  type: journal 
  published: NAACL
  abstract: >

- title: "Black or White but never neutral: How readers perceive identity from yellow or skin-toned emoji"
  authors: Alexander Robertson, Walid Magdy, Sharon Goldwater
  year: 2021 
  month:  10
  type: journal 
  published: Proceedings of the ACM on Human-Computer Interaction
  volume: 5<div class="gsc_oci_field">IssueCSCW2
  pages: 1-23
  publisher: ACM
  abstract: >

- title: "Semantic journeys: quantifying change in emoji meaning from 2012-2018"
  authors: Alexander Robertson, Farhana Ferdousi Liza, Dong Nguyen, Barbara McGillivray, Scott A Hale
  year: 2021 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:2105.00846
  abstract: >

- title: "Scaling Systematic Literature Reviews with Machine Learning Pipelines"
  authors: Seraphina Goldfarb-Tarrant, Alexander Robertson, Jasmina Lazic, Theodora Tsouloufi, Louise Donnison, Karen Smyth
  year: 2020 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2010.04665
  abstract: >

- title: "Identity Signals in Emoji do not Influence Perception of Factual Truth on Twitter"
  authors: Alexander Robertson, Walid Magdy, Sharon Goldwater
  year: 2021 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:2105.03160
  abstract: >

- title: "Expression and perception of identity through skin-toned emoji"
  authors: Alexander Robertson
  year: 2022 
  month:  6
  publisher: The University of Edinburgh
  abstract: >

<div id="gsc_oci_title">Automatic Normalisation of Historical Text"
  authors: Alexander Robertson
  year: 2017
  institution: University of Edinburgh

<div id="gsc_oci_title">Automatic Identification of Spelling Variation in Historical Texts"
  authors: Alexander Robertson
  year: 2015
  institution: University of Cambridge

- title: "Deep Learning for Historical Text Normalisation"
  authors: Alexander Robertson
  abstract: >
Historical texts generally lack a single consistent orthographic form for each lexical item. This results in an inflated number of unique &#8220;words&#8221; and makes it difficult to perform NLP and IR tasks. Such tasks often involve calculating how many times a word appears or at least take for granted the singular identity of a word&#8217;s representation.

- title: "Multi-Scale Attributed Node Embedding"
  authors: Benedek Rozemberczki, Carl Allen, Rik Sarkar
  year: 2021
  type: journal 
  published: Journal of Complex Networks
  volume: 9<div class="gsc_oci_field">Issue2
  publisher: Oxford Academic
  abstract: >
We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighbourhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighbourhood relationships over multiple scales is useful for a range of applications, including latent feature identification across disconnected networks with similar features. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are computationally efficient and outperform comparable models on social networks and web graphs.

- title: "GEMSEC: Graph Embedding with Self-Clustering"
  authors: Benedek Rozemberczki, Ryan Davies, Rik Sarkar, Charles Sutton
  year: 2019
  type: journal 
  published: Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2019
  pages: 65-72
  abstract: >

- title: "Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models"
  authors: Benedek Rozemberczki, Rik Sarkar
  year: 2020 
  month:  5
  type: conference 
  published: Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &#39;20)
  abstract: >

- title: "Scaling Graph Neural Networks with Approximate PageRank"
  authors: 
  year: 2020 
  month:  7
  type: conference 
  published: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
  pages: 2464&#8211;2473
  publisher: ACM
  abstract: >
Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge--many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings.

- title: "Karate Club: An API Oriented Open-Source Python Framework for Unsupervised Learning on Graphs"
  authors: Benedek Rozemberczki, Oliver Kiss, Rik Sarkar
  year: 2020 
  month:  3
  type: conference 
  published: Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &#39;20)
  abstract: >

- title: "Fast-Sequence Based Embedding with Diffusion Graphs"
  authors: Benedek Rozemberczki, Rik Sarkar
  year: 2018/3
  type: conference 
  published: Proceedings of the 9th Conference on Complex Networks CompleNet 2018
  pages: 99-107
  abstract: >
A graph          embedding          is a representation of graph vertices in a low- dimensional space, which approximately preserves properties such as distances between nodes. Vertex sequence-based embedding procedures use features extracted from linear sequences of nodes to create embeddings using a neural network. In this paper, we propose diffusion graphs as a method to rapidly generate vertex sequences for network embedding. Its computational efficiency is superior to previous methods due to simpler sequence generation, and it produces more accurate results. In experiments, we found that the performance relative to other methods improves with increasing edge density in the graph. In a community detection task, clustering nodes in the embedding space produces better results compared to other sequence-based embedding methods.

- title: "PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models"
  authors: 
  year: 2021 
  month:  4
  type: conference 
  published: Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM &#39;21)
  pages: 4564-4573
  abstract: >

- title: "Little Ball of Fur: A Python Library for Graph Sampling"
  authors: Benedek Rozemberczki, Oliver Kiss, Rik Sarkar
  year: 2020 
  month:  6
  type: journal 
  published: Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &#39;20)
  abstract: >

- title: "The Shapley Value in Machine Learning"
  authors: 
  year: 2022 
  month:  2
  type: conference 
  published: Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence Survey Track
  pages: 5572-5579
  abstract: >

- title: "Pathfinder discovery networks for neural message passing"
  authors: Benedek Rozemberczki, Peter Englert, Amol Kapoor, Martin Blais, Bryan Perozzi
  year: 2021 
  month:  4
  type: conference 
  published: Proceedings of the Web Conference 2021
  pages: 2547-2558
  abstract: >
In this work we propose Pathfinder Discovery Networks (PDNs), a method for jointly learning a message passing graph over a multiplex network with a downstream semi-supervised model. PDNs inductively learn an aggregated weight for each edge, optimized to produce the best outcome for the downstream learning task. PDNs are a generalization of attention mechanisms on graphs which allow flexible construction of similarity functions between nodes. They also support edge convolutions and cheap multiscale mixing layers. We show that PDNs overcome weaknesses of existing methods for graph attention (eg Graph Attention Networks), such as the diminishing weight problem.

- title: "Chickenpox Cases in Hungary: a Benchmark Dataset for Spatiotemporal Signal Processing with Graph Neural Networks"
  authors: Benedek Rozemberczki, Paul Scherer, Oliver Kiss, Rik Sarkar, Tamas Ferenci
  year: 2021 
  month:  2
  type: journal 
  published: WWW &#39;21 Graph Learning Benchmarks Workshop
  abstract: >

- title: "The Shapley Value of Classifiers in Ensemble Games"
  authors: Benedek Rozemberczki, Rik Sarkar
  year: 2021 
  month:  1
  type: conference 
  published: Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM &#39;21)
  pages: 1558&#8211;1567
  abstract: >

- title: "Biological Insights Knowledge Graph: an integrated knowledge graph to support drug development"
  authors: David Geleta, Andriy Nikolov, Gavin Edwards, Anna Gogleva, Richard Jackson, Erik Jansson, Andrej Lamov, Sebastian Nilsson, Marina Pettersson, Vladimir Poroshin, Benedek Rozemberczki, Timothy Scrivener, Michael Ughetto, Eliseo Papa
  year: 2021 
  month:  1
  type: journal 
  published: Machine Learning on Graphs Workshop @WSDM 2022
  abstract: >
The use of knowledge graphs as a data source for machine learning methods to solve complex problems in life sciences has rapidly become popular in recent years. Our Biological Insights Knowledge Graph (BIKG) combines relevant data for drug development from public as well as internal data sources to provide insights for a range of tasks: from identifying new targets to repurposing existing drugs. Besides the common requirements to organisational knowledge graphs such as being able to capture the domain precisely and give the users the ability to search and query the data, the focus on handling multiple use cases and supporting use case-specific machine learning models presents additional challenges: the data models must also be streamlined for the performance of downstream tasks; graph content must be easily customisable for different use cases; different projections of the graph content are required to support a wider range of different consumption modes. In this paper we describe our main design choices in implementation of the BIKG graph and discuss different aspects of its life cycle: from graph construction to exploitation.

- title: "Topological Signatures for Fast Mobility Analysis"
  authors: Abhirup Ghosh, Benedek Rozemberczki, Subramanian Ramamoorthy, Rik Sarkar
  year: 2018 
  month:  11
  type: conference 
  published: Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems
  pages: 159-168
  abstract: >

- title: "A Unified View of Relational Deep Learning for Drug Pair Scoring"
  authors: Benedek Rozemberczki, Stephen Bonner, Andriy Nikolov, Michael Ughetto, Sebastian Nilsson, Eliseo Papa
  year: 2021 
  month:  11
  type: conference 
  published: Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence Survey Track
  pages: 5564-5571
  abstract: >

- title: "Twitch Gamers: a Dataset for Evaluating Proximity Preserving and Structural Role-based Node Embeddings"
  authors: Benedek Rozemberczki, Rik Sarkar
  year: 2021/1
  type: journal 
  published: Workshop on Graph Learning Benchmarks@ TheWebConf 2021
  abstract: >

- title: "Explainable Biomedical Recommendations via Reinforcement Learning Reasoning on Knowledge Graphs"
  authors: Gavin Edwards, Sebastian Nilsson, Benedek Rozemberczki, Eliseo Papa
  year: 2022
  type: journal 
  published: Machine Learning on Graphs Workshop @WSDM 2022
  abstract: >

- title: "Persistence and Performance in Co-Enrollment Network Embeddings: An Empirical Validation of Tinto&#39;s Student Integration Model"
  authors: 
  year: 2021 
  month:  2
  type: journal 
  published: IEEE Transactions on Learning Technologies
  volume: 14<div class="gsc_oci_field">Issue1
  pages: 106-121
  publisher: IEEE
  abstract: >

- title: "ChemicalX: A Deep Learning Library for Drug Pair Scoring"
  authors: Benedek Rozemberczki, Charles Tapley Hoyt, Anna Gogleva, Piotr Grabowski, Klas Karis, Andrej Lamov, Andriy Nikolov, Sebastian Nilsson, Michael Ughetto, Yu Wang, Tyler Derr, Benjamin M Gyori
  year: 2022 
  month:  2
  type: conference 
  published: Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining
  pages: 3819&#8211;3828
  abstract: >

- title: "MOOMIN: Deep Molecular Omics Network for Anti-Cancer Drug Combination Therapy"
  authors: Benedek Rozemberczki, Anna Gogleva, Sebastian Nilsson, Gavin Edwards, Andriy Nikolov, Eliseo Papa
  year: 2021 
  month:  10
  type: journal 
  published: Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM &#39;22)
  abstract: >

- title: "Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings"
  authors: Philippa Shoemark, Farhana Ferdousi Liza, Dong Nguyen, Scott Hale, Barbara McGillivray
  year: 2019/11
  type: conference 
  published: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
  pages: 66-76
  abstract: >

- title: "Toward crowdsourcing micro-level behavior annotations: the challenges of interface, training, and generalization"
  authors: Sunghyun Park, Philippa Shoemark, Louis-Philippe Morency
  year: 2014 
  month:  2
  type: conference 
  published: Proceedings of the 19th international conference on Intelligent User Interfaces
  pages: 37-46
  publisher: ACM
  abstract: >

- title: "Aye or naw, whit dae ye hink? Scottish independence and linguistic identity on social media"
  authors: Philippa Shoemark, Debnil Sur, Luke Shrimpton, Iain Murray, Sharon Goldwater
  year: 2017/4
  type: conference 
  published: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers
  pages: 1239-1248
  abstract: >
Political surveys have indicated a relationship between a sense of Scottish identity and voting decisions in the 2014 Scottish Independence Referendum. Identity is often reflected in language use, suggesting the intuitive hypothesis that individuals who support Scottish independence are more likely to use distinctively Scottish words than those who oppose it. In the first large-scale study of sociolinguistic variation on social media in the UK, we identify distinctively Scottish terms in a data-driven way, and find that these terms are indeed used at a higher rate by users of pro-independence hashtags than by users of anti-independence hashtags. However, we also find that in general people are less likely to use distinctively Scottish words in tweets with referendum-related hashtags than in their general Twitter activity. We attribute this difference to style shifting relative to audience, aligning with previous work showing that Twitter users tend to use fewer local variants when addressing a broader audience.

- title: "Towards robust cross-linguistic comparisons of phonological networks"
  authors: Philippa Shoemark, Sharon Goldwater, James Kirby, Rik Sarkar
  year: 2016
  type: journal 
  published: Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology
  pages: 110-120
  abstract: >
Recent work has proposed using network science to analyse the structure of the mental lexicon by viewing words as nodes in a phonological network, with edges connecting words that differ by a single phoneme. Comparing the structure of phonological networks across different languages could provide insights into linguistic typology and the cognitive pressures that shape language acquisition, evolution, and processing. However, previous studies have not considered how statistics gathered from these networks are affected by factors such as lexicon size and the distribution of word lengths. We show that these factors can substantially affect the statistics of a phonological network and propose a new method for making more robust comparisons. We then analyse eight languages, finding many commonalities but also some qualitative differences in their lexicon structure.

- title: "Topic and audience effects on distinctively Scottish vocabulary usage in Twitter data"
  authors: Philippa Shoemark, James Kirby, Sharon Goldwater
  year: 2017
  type: journal 
  published: Proceedings of the Workshop on Stylistic Variation
  pages: 59-68
  abstract: >
Sociolinguistic research suggests that speakers modulate their language style in response to their audience. Similar effects have recently been claimed to occur in the informal written context of Twitter, with users choosing less region-specific and non-standard vocabulary when addressing larger audiences. However, these studies have not carefully controlled for the possible confound of topic: that is, tweets addressed to a broad audience might also tend towards topics that engender a more formal style. In addition, it is not clear to what extent previous results generalize to different samples of users. Using mixed-effects models, we show that audience and topic have independent effects on the rate of distinctively Scottish usage in two demographically distinct Twitter user samples. However, not all effects are consistent between the two groups, underscoring the importance of replicating studies on distinct user samples before drawing strong conclusions from social media data.

- title: "Inducing a lexicon of sociolinguistic variables from code-mixed text"
  authors: Philippa Shoemark, James Kirby, Sharon Goldwater
  year: 2018
  type: journal 
  published: Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text
  pages: 1-6
  abstract: >
Sociolinguistics is often concerned with how variants of a linguistic item (eg, nothing vs. nothin&#8217;) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (football&#8594; British, fitba&#8594; Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.

- title: "Cross-linguistic network structure effects on non-word acceptability judgements"
  authors: Philippa Shoemark
  year: 2013
  institution: MA dissertation, University of Edinburgh
  abstract: >
Speakers of a given language are intuitively able to distinguish between sound sequences which happen not to be words in their language but conceivably could be, and others which could not.In early generative phonology, a tripartite categorical distinction was drawn between occurring word forms, such as/brIk/in English; non-occurring but admissible forms, such as/blIk/, which became known as accidental gaps; and non-occurring but inadmissible forms, such as/bnIk/, which became known as systematic gaps. In order to account for how a child who hears/brIk/but not/blIk/or/bnIk/learns to categorise/blIk/as an accidental gap and/bnIk/as a systematic gap, Chomsky and Halle [8] posited that the child constructs a grammar containing an intricate set of ordered rules which generate/blIk/but are violated by/bnIk/.

- title: "Discovering and analysing lexical variation in social media text"
  authors: Philippa Jane Shoemark
  year: 2020 
  month:  6
  publisher: The University of Edinburgh
  abstract: >

- title: "Comparing phonological networks"
  authors: Philippa Shoemark
  abstract: >
The mental lexicon can be modelled as a network in which phonologically similar words are connected to one another. Examining the structural properties of phonological networks can reveal complex relationships among words, and studies combining network analysis with psycholinguistic experiments suggest that such relationships influence lexical processing. Comparing structural properties of phonological networks across languages and across different stages of language acquisition could potentially help to answer important questions about linguistic typology and the cognitive pressures that shape the acquisition and evolution of human language&#8212;but previous studies comparing phonological network structure across different lexicons have failed to account for confounding factors such as differences in the sizes of the lexicons and the inclusion or exclusion of inflectional variants. We show that these factors can substantially affect the statistics of a phonological network, and develop new methodologies for comparing phonological networks in more robust and informative ways. i

- title: "Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work?"
  authors: Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, Samuel R Bowman
  year: 2020 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:2005.00628
  abstract: >

- title: "CrowS-pairs: A challenge dataset for measuring social biases in masked language models"
  authors: Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R Bowman
  year: 2020 
  month:  9
  type: journal 
  published: arXiv preprint arXiv:2010.00133
  abstract: >

- title: "From Characters to Words to in Between: Do We Capture Morphology?"
  authors: Clara Vania, Adam Lopez
  year: 2017 
  month:  4
  type: journal 
  published: arXiv preprint arXiv:1704.08352
  abstract: >

- title: "English intermediate-task training improves zero-shot cross-lingual transfer too"
  authors: Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Samuel R Bowman
  year: 2020 
  month:  5
  type: journal 
  published: arXiv preprint arXiv:2005.13013
  abstract: >

- title: "Sentiment Lexicon Generation for an Under-Resourced Language"
  authors: Clara Vania, Moh. Ibrahim, Mirna Adriani
  year: 2014/1
  type: journal 
  published: Int. J. Comput. Linguistics Appl.
  volume: 5<div class="gsc_oci_field">Issue1
  pages: 59-72
  abstract: >
Sentiment analysis and opinion mining are actively explored nowadays. One of the most important resources for the sentiment analysis task is sentiment lexicon. This paper presents our study in building domain-specific sentiment lexicon for Indonesian language. Our main contributions are (1) methods to expand sentiment lexicon using sentiment patterns and (2) a technique to classify the polarity of a word using the sentiment score. Our method is able to generate sentiment lexicon automatically by using a small seed of sentiment words, user reviews, and part-ofspeech (POS) tagger. We develop the lexicon for Indonesian language using a set of seed words translated from English sentiment lexicon and expand them using sentiment patterns found in the user reviews. Our results show that the proposed method can generate additional lexicon with sentiment accuracy of 77.7%.

- title: "A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages"
  authors: 
  year: 2019 
  month:  9
  type: journal 
  published: arXiv preprint arXiv:1909.02857
  abstract: >

- title: "Automatically building a corpus for sentiment analysis on Indonesian tweets"
  authors: Alfan Farizki Wicaksono, Clara Vania, Bayu Distiawan, Mirna Adriani
  year: 2014/12
  type: conference 
  published: Proceedings of the 28th Pacific Asia conference on language, information and computing
  pages: 185-194
  abstract: >
The popularity of the user generated content, such as Twitter, has made it a rich source for the sentiment analysis and opinion mining tasks. This paper presents our study in automatically building a training corpus for the sentiment analysis on Indonesian tweets. We start with a set of seed sentiment corpus and subsequently expand them using a classifier model whose parameters are estimated using the Expectation and Maximization (EM) framework. We apply our automatically built corpus to perform two tasks, namely opinion tweet extraction and tweet polarity classification using various machine learning approaches. Experiment result shows that a classifier model trained on our data, which is automatically constructed using our proposed method, outperforms the baseline system in terms of opinion tweet extraction and tweet polarity classification.

<div id="gsc_oci_title">Multilingual Probing Tasks for Word Representations"
  authors: 
  year: 2020
  type: journal 
  published: Computational Linguistics<div class="gsc_oci_field">IssueJust Accepted
  pages: 1-92
  publisher: MIT Press

- title: "What do character-level models learn about morphology? The case of dependency parsing"
  authors: Clara Vania, Andreas Grivas, Adam Lopez
  year: 2018
  type: conference 
  published: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
  publisher: Association for Computational Linguistics
  abstract: >

- title: "Comparing test sets with item response theory"
  authors: Clara Vania, Phu Mon Htut, William Huang, Dhara Mungra, Richard Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, Samuel R Bowman
  year: 2021 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2106.00840
  abstract: >

- title: "University of Indonesia at TREC 2011 Microblog Task."
  authors: Samuel Louvan, Mochamad Ibrahim, Mirna Adriani, Clara Vania, Bayu Distiawan, Metti Zakaria Wanagiri
  year: 2011 
  month:  12
  type: conference 
  published: TREC
  abstract: >
In this paper we describe our submission to the TREC2011 MicroblogTrack. Our run combines different methods namely customized scoring function, query reformulation, and query expansion. We apply query expansion from dataset with different weighting scheme. Furthermore, we do an initial experiment to incorporate timestamp of the tweet document in order to improve search performance. We found the query expansion utilizing external search result combined with re-tweet value in the customized scoring function was the most effective.

- title: "What ingredients make for an effective crowdsourcing protocol for difficult NLU data collection tasks?"
  authors: Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara Vania, Samuel R Bowman
  year: 2021 
  month:  6
  type: journal 
  published: arXiv preprint arXiv:2106.00794
  abstract: >

- title: "Asking crowdworkers to write entailment examples: The best of bad options"
  authors: Clara Vania, Ruijie Chen, Samuel R Bowman
  year: 2020 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2010.06122
  abstract: >

- title: "On understanding character-level models for representing morphology"
  authors: Clara Vania
  year: 2020 
  month:  1
  publisher: The University of Edinburgh
  abstract: >

- title: "Automatic external plagiarism detection using passage similarities"
  authors: Clara Vania, Mirna Adriani
  year: 2010 
  month:  1
  type: journal 
  published: Braschler and Harman
  abstract: >
In this paper, we report our approach in detecting external plagiarism. For the pre-processing stage, we identify non-English documents and translate them into English using an online translator tool. Then we index and retrieve the top documents that are similar to the suspicious documents. We divide the retrieved documents into passages where each passage contains twenty sentences. The plagiarism is detected by identifying the number of overlapped words between suspicious and source passages.

- title: "Sigmorphon 2021 shared task on morphological reinflection: Generalization across languages"
  authors: 
  year: 2021/8
  type: conference 
  published: Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology
  pages: 229-259
  abstract: >

- title: "VisualSem: a high-quality knowledge graph for vision and language"
  authors: Houda Alberts, Teresa Huang, Yash Deshpande, Yibo Liu, Kyunghyun Cho, Clara Vania, Iacer Calixto
  year: 2020 
  month:  8
  type: journal 
  published: arXiv preprint arXiv:2008.09150
  abstract: >

- title: "UParse: the Edinburgh system for the CoNLL 2017 UD shared task"
  authors: Clara Vania, Xingxing Zhang, Adam Lopez
  year: 2017/8
  type: conference 
  published: Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies
  pages: 100-110
  abstract: >
This paper presents our submissions for the CoNLL 2017 UD Shared Task. Our parser, called UParse, is based on a neural network graph-based dependency parser. The parser uses features from a bidirectional LSTM to to produce a distribution over possible heads for each word in the sentence. To allow transfer learning for low-resource treebanks and surprise languages, we train several multilingual models for related languages, grouped by their genus and language families. Out of 33 participants, our system achieves rank 9th in the main results, with 75.49 UAS and 68.87 LAS F-1 scores (average across 81 treebanks).

- title: "IndoNLI: A natural language inference dataset for Indonesian"
  authors: Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan, Fahrurrozi Rahman, Clara Vania
  year: 2021 
  month:  10
  type: journal 
  published: arXiv preprint arXiv:2110.14566
  abstract: >

- title: "The effect of syllable and word stress on the quality of Indonesian HMM-based speech synthesis system"
  authors: Clara Vania, Mirna Adriani
  year: 2011 
  month:  12
  type: conference 
  published: 2011 International Conference on Advanced Computer Science and Information Systems
  pages: 413-418
  publisher: IEEE
  abstract: >

- title: "Disentangling Style Factors from Speaker Representations"
  authors: Jennifer Williams, Simon King
  year: 2019
  type: journal 
  published: Proc. Interspeech 2019
  pages: 3945-3949
  abstract: >

- title: "DNN Multimodal Fusion Techniques for Predicting Video Sentiment"
  authors: Jennifer Williams, Ramona Comanescu, Oana Radu, Leimin Tian
  year: 2018 
  month:  7
  type: journal 
  published: ACL 2018
  pages: 64
  abstract: >
We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK. Previous work on multimodal sentiment analysis have been focused on input-level feature fusion or decision-level fusion for multimodal fusion. Here, we propose an intermediate-level feature fusion, which merges weights from each modality (audio, video, and text) during training with subsequent additional training. Moreover, we tested principle component analysis (PCA) for feature selection. We found that applying PCA increases unimodal performance, and multimodal fusion outperforms unimodal models. Our experiments show that our proposed intermediate-level feature fusion outperforms other fusion techniques, and it achieves the best performance with an overall binary accuracy of 74.0% on video+ text modalities. Our work also improves feature selection for unimodal sentiment analysis, while proposing a novel and effective multimodal fusion architecture for this task.

- title: "Recognizing Emotions in Video Using Multimodal DNN Feature Fusion"
  authors: Jennifer Williams, Steven Kleinegesse, Ramona Comanescu, Oana Radu
  year: 2018
  type: journal 
  published: ACL 2018 Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)
  pages: 11-19
  abstract: >
We present our system description of input-level multimodal fusion of audio, video, and text for recognition of emotions and their intensities for the 2018 First Grand Challenge on Computational Modeling of Human Multimodal Language. Our proposed approach is based on input-level feature fusion with sequence learning from Bidirectional Long-Short Term Memory (BLSTM) deep neural networks (DNNs). We show that our fusion approach outperforms unimodal predictors. Our system performs 6-way simultaneous classification and regression, allowing for overlapping emotion labels in a video segment. This leads to an overall binary accuracy of 90%, overall 4-class accuracy of 89.2% and an overall mean-absolute-error (MAE) of 0.12. Our work shows that an early fusion technique can effectively predict the presence of multi-label emotions as well as their coarse-grained intensities. The presented multimodal approach creates a simple and robust baseline on this new Grand Challenge dataset. Furthermore, we provide a detailed analysis of emotion intensity distributions as output from our DNN, as well as a related discussion concerning the inherent difficulty of this task.

- title: "Extracting and Modeling Durations for Habits and Events from Twitter"
  authors: Jennifer Williams, Graham Katz
  year: 2012 
  month:  7
  type: conference 
  published: ACL 2012
  pages: 223-227
  publisher: Association for Computational Linguistics
  abstract: >
We seek to automatically estimate typical durations for events and habits described in Twitter tweets. A corpus of more than 14 million tweets containing temporal duration information was collected. These tweets were classified as to their habituality status using a bootstrapped, decision tree. For each verb lemma, associated duration information was collected for episodic and habitual uses of the verb. Summary statistics for 483 verb lemmas and their typical habit and episode durations has been compiled and made available. This automatically generated duration information is broadly comparable to hand-annotation.

- title: "Twitter Language Identification Of Similar Languages And Dialects Without Ground Truth"
  authors: Jennifer Williams, Charlie K. Dagli
  year: 2017
  type: conference 
  published: EACL 2017 VarDial Workshop
  pages: 73&#8211;83
  publisher: Association for Computational Linguistics (ACL)
  abstract: >
We present a new method to bootstrap filter Twitter language ID labels in our dataset for automatic language identification (LID). Our method combines geo-location, original Twitter LID labels, and Amazon Mechanical Turk to resolve missing and unreliable labels. We are the first to compare LID classification performance using the MIRA algorithm and langid. py. We show classifier performance on different versions of our dataset with high accuracy using only Twitter data, without ground truth, and very few training examples. We also show how Platt Scaling can be use to calibrate MIRA classifier output values into a probability distribution over candidate classes, making the output more intuitive. Our method allows for fine-grained distinctions between similar languages and dialects and allows us to rediscover the language composition of our Twitter dataset.

- title: "A Language-Independent Approach to Automatic Text Difficulty Assessment for Second-Language Learners"
  authors: Wade Shen, Jennifer Williams, Tamas Marius, Salesky E
  year: 2013 
  month:  8
  type: conference 
  published: ACL 2013 PITR Workshop
  pages: 30
  abstract: >
In this paper, we introduce a new baseline for language-independent text difficulty assessment applied to the Interagency Language Roundtable ILR proficiency scale. We demonstrate that reading level assessment is a discriminative problem that is best-suited for regression. Our baseline uses z-normalized shallow length features and TF-LOG weighted vectors on bag-of-words for Arabic, Dari, English, and Pashto. We compare Support Vector Machines and the Margin-Infused Relaxed Algorithm measured by mean squared error. We provide an analysis of which features are most predictive of a given level.Descriptors:

- title: "Comparison of Speech Representations for Automatic Quality Estimation in Multi-Speaker Text-to-Speech Synthesis"
  authors: Jennifer Williams, Joanna Rownicka, Pilar Oplustil, Simon King
  year: 2020 
  month:  2
  type: journal 
  published: Speaker Odyssey Workshop, 2020
  abstract: >

- title: "Speech is Silver, Silence is Golden: What do ASVspoof-trained Models Really Learn?"
  authors: 
  year: 2021 
  month:  6
  type: journal 
  published: 2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge
  pages: 55-60
  abstract: >

- title: "Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction"
  authors: Yi Zhao, Haoyu Li, Cheng-I Lai, Jennifer Williams, Erica Cooper, Junichi Yamagishi
  year: 2020 
  month:  5
  type: journal 
  published: Proc. Interspeech 2020
  abstract: >

- title: "Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features"
  authors: Jennifer Williams, Joanna Rownicka
  year: 2019
  type: journal 
  published: Proc. Interspeech 2019
  pages: 1053-1057
  abstract: >

- title: "Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm"
  authors: Jennifer Williams, Yi Zhao, Erica Cooper, Junichi Yamagishi
  year: 2021 
  month:  6
  type: conference 
  published: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  pages: 7053-7057
  publisher: IEEE
  abstract: >

- title: "Extracting Fine-Grained Durations for Verbs from Twitter"
  authors: Jennifer Williams
  year: 2012 
  month:  7
  type: conference 
  published: ACL 2012 (SRW)
  pages: 49-54
  publisher: Association for Computational Linguistics
  abstract: >
This paper presents recent work on a new method to automatically extract finegrained duration information for common verbs using a large corpus of Twitter tweets. Regular expressions were used to extract verbs and durations from each tweet in a corpus of more than 14 million tweets with 90.38% precision covering 486 verb lemmas. Descriptive statistics for each verb lemma were found as well as the most typical fine-grained duration measure. Mean durations were compared with previous work by Gusev et al.(2011) and it was found that there is a small positive correlation.

- title: "Human Perception of Audio Deepfakes"
  authors: 
  year: 2021 
  month:  7
  type: journal 
  published: First International Workshop on Deepfake Detection for Audio Multimedia at ACM Multimedia 2022
  abstract: >

- title: "An Unsupervised Method to Select a Speaker Subset from Large Multi-Speaker Speech Synthesis Datasets"
  authors: Pilar Oplustil Gallegos, Jennifer Williams, Joanna Rownicka, Simon King
  year: 2020
  abstract: >
Large multi-speaker datasets for TTS typically contain diverse speakers, recording conditions, styles and quality of data. Although one might generally presume that more data is better, in this paper we show that a model trained on a carefully-chosen subset of speakers from LibriTTS provides significantly better quality synthetic speech than a model trained on a larger set. We propose an unsupervised methodology to find this subset by clustering per-speaker acoustic representations.

- title: "Exploring Disentanglement with Multilingual and Monolingual VQ-VAE"
  authors: Jennifer Williams, Jason Fong, Erica Cooper, Junichi Yamagishi
  year: 2021 
  month:  5
  type: conference 
  published: Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)
  pages: 124--129
  abstract: >

- title: "A New Twitter Verb Lexicon for Natural Language Processing."
  authors: Jennifer Williams, Graham Katz
  year: 2012
  type: conference 
  published: LREC 2012
  pages: 293-298
  abstract: >
We describe in-progress work on the creation of a new lexical resource that contains a list of 486 verbs annotated with quantified temporal durations for the events that they describe. This resource is being compiled from more than 14 million tweets from the Twitter microblogging site. We are creating this lexicon of verbs and typical durations to address a gap in the available information that is represented in existing research. The data that is contained in this lexical resource is unlike any existing resources, which have been traditionally comprised from literature excerpts, news stories, and full-length weblogs. The kind of knowledge about how long an event lasts is crucial for natural language processing and is especially useful when the temporal duration of an event is implied. We are using data from Twitter because Twitter is a rich resource since people are publicly posting about real events and real durations of those events throughout the day.

- title: "Revisiting Speech Content Privacy"
  authors: 
  year: 2021 
  month:  9
  type: journal 
  published: 1st ISCA Symposium of the Security &amp; Privacy in Speech Communication
  abstract: >

- title: "Finding Good Enough: A Task-Based Evaluation of Query Biased Summarization for Cross-Language Information Retrieval"
  authors: Jennifer Williams, Sharon Tam, Wade Shen
  year: 2014
  type: journal 
  published: Empirical Methods in Natural Language Processing (EMNLP)
  abstract: >
In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval (CLIR) using relevance prediction. We describe our 13 summarization methods each from one of four summarization strategies. We show how well our methods perform using Farsi text from the CLEF 2008 shared-task, which we translated to English automtatically. We report precision/recall/F1, accuracy and time-on-task. We found that different summarization methods perform optimally for different evaluation metrics, but overall query biased word clouds are the best summarization strategy. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating muchneeded evaluation standards and datasets.

- title: "Attacker Attribution of Audio Deepfakes"
  authors: 
  year: 2022 
  month:  3
  type: journal 
  published: Interspeech 2022
  pages: 2788-2
  abstract: >

- title: "Analysing Temporal Sensitivity of VQ-VAE Sub-Phone Codebooks"
  authors: Jason Fong, Jennifer Williams, Simon King
  year: 2021
  type: conference 
  published: Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)
  pages: 227-231
  abstract: >
In this work we present an analysis of temporal sensitivity of VQ-VAE sub-phone token sequences. Previous work has demonstrated that VQ-VAE systems learn a type of sub-phone representation. However, a detailed examination of the representations themselves is currently lacking. We address this gap by exploring linguistic unit reorganisation. Our experiments show that sub-phone codebook sequences are temporally correlated enough to identify VQ codes that correspond to distinct linguistic units. We found that it is possible to extract VQ codes and re-arrange these linguistic units in a meaningful way (ie changing the word-order of a sentence). This work puts us one step closer to understanding how to modify pronunciations at a fine granularity, such as below the phone-level unit.

